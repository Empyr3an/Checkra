{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "from checkra import happyscribe_scrape as happy\n",
    "\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "lexurl=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "billionaireurl = \"/public/we-study-billionaires-the-investor-s-podcast-network\"\n",
    "lexhtml_location = \"happyscribe_Lex_Fridman_html.txt\"\n",
    "billhtml_location=\"happyscribe_billionaires_podcast.txt\"\n",
    "happy.update_transcripts(billhtml_location, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from checkra.nlp_pipeline import nlp, run_pipeline\n",
    "from checkra.text_clean import text_fix, strip_accents\n",
    "doc = nlp(text_fix(open(\"data/We_Study_Billionaires_-_The_Investor’s_Podcast_Network/TIP325|Real_Estate_Investing_with_BiggerPockets_(Business_Podcast).txt\").read()))\n",
    "# doc = nlp(text_fix(open(\"3Lex/#136|Dan_Carlin|Hardcore_History.txt\").read()))\n",
    "# print(doc.user_data[\"stamps\"])\n",
    "# print(doc.user_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = run_pipeline(\"data/3Lex\", \"Lex Fridman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1 = run_pipeline(\"data/8Lex\", \"Lex Fridman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2 = run_pipeline(\"data/We_Study_Billionaires_-_The_Investor’s_Podcast_Network\", \"Investors Podcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, doc in enumerate(docs2):\n",
    "    print(ind, doc.user_data[\"guest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].user_data[\"traits\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "count = 0\n",
    "not_processed = []\n",
    "for ind, file in enumerate(os.listdir(\"7Lex\")):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file2 = file.replace(\"_\", \" \").replace(\"|\", \" \").replace(\".txt\", \" \")\n",
    "    #     print(ind, file) \n",
    "        processed = False\n",
    "\n",
    "\n",
    "        for doc in docs:\n",
    "            if doc.user_data[\"guest\"] in file2 and doc.user_data[\"title\"] in file2:\n",
    "    #             os.rename(\"5Lex/\"+file, \"7Lex/\"+file)\n",
    "                \n",
    "                processed = True\n",
    "                count+=1\n",
    "                break\n",
    "        if processed:\n",
    "            os.rename(\"7Lex/\"+file, \"8Lex/\"+file)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import urllib\n",
    "\n",
    "username = urllib.parse.quote_plus('Empyr3an')\n",
    "password = urllib.parse.quote_plus(\"B4ldr1c7@1\")\n",
    "\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://{}:{}@cluster0.4pec2.mongodb.net/Checkra?retryWrites=true&w=majority\".format(username, password))\n",
    "db = cluster[\"production\"]\n",
    "\n",
    "# db.create_collection(\"lmini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"Lex Fridman Podcast\"].drop()\n",
    "\n",
    "db[\"Lex Fridman Podcast\"].insert_many([doc.user_data for doc in docs])\n",
    "db[\"Lex Fridman Podcast\"].insert_many([doc.user_data for doc in docs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"The Investor's Podcast\"].drop()\n",
    "db[\"The Investor's Podcast\"].insert_many([doc.user_data for doc in docs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_atts = []\n",
    "for doc in list(collection.find({\"books\":{\"$ne\":[]}})):\n",
    "    print(doc[\"guest\"], doc[att])\n",
    "    for a in doc[att]:\n",
    "        all_atts.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "\n",
    "    doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "# with open(\"cucumber/Lexnumbered.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "#     f.write(doc_bin.to_bytes())\n",
    "\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
