{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 1.37 s, total: 17.3 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#import in all libraries and methods\n",
    "%run -i 'myimports.py'\n",
    "%run -i 'datagathering.py'\n",
    "%run -i 'nlp_processing.py' #imports packages and methods\n",
    "%run -i 'timestamp_generation.py'\n",
    "\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "podcast_host = \"Lex Fridman\"\n",
    "html_location = \"Archive/happyscribe_Lex_Fridman_html.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 58.6 ms, total: 187 ms\n",
      "Wall time: 385 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "username = urllib.parse.quote_plus('Empyr3an')\n",
    "password = urllib.parse.quote_plus(\"B4ldr1c7@1\")\n",
    "\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://{}:{}@cluster0.4pec2.mongodb.net/Checkra?retryWrites=true&w=majority\".format(username, password))\n",
    "db = cluster[\"podcasts\"]\n",
    "collection = db[\"lex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5fe6859cc9a88b14eadc764f'), 'places': ['Tehran', 'Vancouver', 'Africa', 'Bethesda', 'London', 'SIM City', 'Ethiopia', 'Proteau Ajai', 'AZ', 'Katrina', 'Mars', 'D.C.', 'Najai', 'Las Vegas', 'Earth', 'New York', 'Canterbury', 'the United States', 'Abu Ghraib', 'Washington', 'Exito', 'Southern California', 'Hong Kong', 'China', 'North Philly', 'Silicon Valley', 'Venezuela', 'Koven', 'Louisiana', 'Tensas', 'Iran', 'Ethereum', 'Alpha Centauri', 'Shenzhen', 'Congo', 'AI', 'South America', 'Turing', 'Cisco', 'Sofia', 'Dharma', 'Philadelphia', 'California', 'Addis Ababa', 'Winton', 'Giannakou', 'Seattle', 'South China', 'Lithuania', 'Austin', 'DC', 'Japan', 'Tawang', 'US', 'Eastern Europe', 'Russia', 'Germany', 'KADEK', 'Prague', 'Bitcoin', 'Tenzer', 'Texas', 'Australia', 'Silicon Alley', 'Asia', 'Alibaba', 'Europe', 'New Zealand', 'the Solaris Superintelligent Ocean', 'Iraq', 'Iceland', 'Poland', 'St. Petersburg', 'Baulderstone']}\n",
      "{'_id': ObjectId('5fe685a7c9a88b14eadc7650'), 'places': ['the Garden of Eden', 'Casablanca', 'Africa', 'Syria', 'Mars', 'New York', 'the United States', 'Jordan Kamangar', 'China', 'Iran', 'Charlottesville', 'Israel', 'Japan', 'India', 'Egypt', 'US', 'United East', 'Bitcoin', 'Jerusalem', 'Paris', 'Asia', 'Jordan Harbour', 'Europe', 'Jordan', 'West', 'Thermopylae', 'the Roman Empire', 'Lisbon']}\n",
      "{'_id': ObjectId('5fe685b6c9a88b14eadc7651'), 'places': ['Rochester', 'USA', 'Abidjan', 'Hollywood', 'Maghreb', 'Minsk', 'Porgera', 'Chernobyl', 'Coplan', 'Gagarin', 'France', 'New York', 'the United States of America', 'the United States', 'States', 'the Soviet Union Europe', 'Amitay', 'China', 'Amort River', 'Chicago', 'aurora borealis', 'Miami Beach', 'French Empire', 'Bidle', 'Kiev', 'the Soviet Union', 'Cincinnati', 'Three Mile Island', 'Soviet Union', 'Potiskum', 'The Soviet Union', 'British Empire', 'Kurchatov', 'Philadelphia', 'Levithan', 'the Soviet Union and America', 'Algeria', 'the Middle East', 'Lagos', 'Sahara', 'Israel', 'Ukraine', 'Helidon', 'Japan', 'Russia', 'Germany', 'United States of America', 'Leningrad', 'Sputnik', 'Moscow', 'Néstor Gazal', 'Clearlake', 'Europe', 'Eurail', 'Crimea', 'Berlin', 'Illinois', 'Poythress', 'Hokkaido', 'Koff', 'us', 'St. Petersburg', 'Libya', 'Miramichi', 'America', 'Saraf']}\n",
      "{'_id': ObjectId('5fe685c0c9a88b14eadc7652'), 'places': ['SIM City', 'Chestnut Hill', 'West', 'Kashef', 'Europa', 'Bitcoin']}\n",
      "{'_id': ObjectId('5fe685cec9a88b14eadc7653'), 'places': ['Arizona', 'Mars', 'earth', 'Earth', 'New York', 'the Soviet Union', 'kingdom', 'Germanys', 'Eastern Germany', 'AI', 'Turing', 'Res Extenda', 'the Eastern Europe', 'US', 'Russia', 'Germany', 'Bitcoin', 'Martey', 'Europe', 'East Germany', 'West', 'Birmingham', 'us', 'America']}\n",
      "{'_id': ObjectId('5fe685dec9a88b14eadc7654'), 'places': ['Yarran', 'Hollywood', 'The United States', 'Somalia', 'Mars', 'New York', 'the United States', 'Singapore', 'Hong Kong', 'Taiwan', 'Eyeblink', 'Chicago', 'the Black Sea', 'L.A.', 'Iran', 'UK', 'the Soviet Union', 'Ledet', 'Soviet Union', 'The Soviet Union', 'South Korea', 'Los Angeles', 'Israel', 'Ukraine', 'Mexico', 'Austin', 'US', 'Russia', 'Moscow', 'Blanca', 'Asia', 'Europe', 'Steve Jobs', 'Rome', 'St. Petersburg', 'America']}\n",
      "{'_id': ObjectId('5fe685edc9a88b14eadc7655'), 'places': ['Sopan', 'St. Louis', 'Brazil', 'Dagestan', 'Mount', 'Philly', 'Costa Rica', 'Vegas', 'New York', 'Bradenton Beach', 'the United States', 'Indianapolis', 'Earth', 'Argentina', 'the Rocky One', 'Chicago', 'L.A.', 'Florida', 'Pennsylvania', 'Brooklyn', 'New England city', 'Iran', 'Cyprus', 'Ireland', 'the Mongol Empire', 'Alabama', 'Buenos', 'California', 'South Boston', 'the Middle East', 'Denver', 'Boston', 'B.C.', 'Doordarshan', 'Japan', 'Leotta', 'Russia', 'Moscow', 'Jordan', 'the West Coast', 'San Francisco', 'May.', 'the East Coast', 'America', 'New England']}\n",
      "{'_id': ObjectId('5fe685f9c9a88b14eadc7656'), 'places': ['London', 'Toronto', 'Berkely', 'Cesc', 'I.B.M.', 'Kashyap', 'R.M.', 'Cisco', 'California', 'Israel', 'US', 'Marion', 'Bitcoin', 'perf', 'Alibaba', 'Eurail', 'Iraq', 'Habana', 'San Francisco', 'Berkeley', 'America']}\n",
      "{'_id': ObjectId('5fe68604c9a88b14eadc7657'), 'places': ['Asia', 'Quito', 'Egypt', 'States', 'AEI', 'Patrón', 'Earth', 'Russia', 'coauthoring', 'AI', 'Turing']}\n",
      "{'_id': ObjectId('5fe68614c9a88b14eadc7658'), 'places': ['Switzerland', 'Mars', 'Narcan', 'Earth', 'Massachusetts', 'the United States', 'Amitay', 'China', 'Silicon Valley', 'Staten Island', 'turkey', 'Kashyap', 'Iran', 'Houston', 'Turing', 'New Jersey', 'Everest', 'Denver', 'Cal Newport', 'Austin', 'US', 'Russia', 'Bitcoin', 'Baltimore', 'Ozma', 'Europe', 'San Pedro', 'Leks']}\n",
      "{'_id': ObjectId('5fe68621c9a88b14eadc7659'), 'places': ['Hollywood', 'Red Circle', 'Russian banya', 'Earth', 'the United States', 'melanopsin', 'Silicon Valley', 'Mullard', 'Dublin', 'Turing', 'neuralink', 'Heights', 'Boston', 'Mexico', 'this Russian Albania', 'Guadaloupe Island', 'Bitcoin', 'Australia', 'Cambridge', 'MT', 'Albania', 'kaaa', 'Berkeley']}\n",
      "{'_id': ObjectId('5fe6862ec9a88b14eadc765a'), 'places': ['Earth', 'Cambridge', 'Nisko', 'Kupets', 'B.C.', 'the Soviet Union', 'Kashef', 'earth', 'US', 'the United States', 'Turing']}\n",
      "{'_id': ObjectId('5fe68639c9a88b14eadc765b'), 'places': ['Earth', 'Australia', 'Siberia', 'Europe', 'SIM City', 'Petersburg', 'Melbourne', 'West', 'the Soviet Union', 'Antarctica', 'US', 'Bitcoin', 'AI', 'America']}\n",
      "{'_id': ObjectId('5fe68644c9a88b14eadc765c'), 'places': ['Vegas', 'Bitcoin', 'the Alpha Star', 'Patrón']}\n",
      "{'_id': ObjectId('5fe68650c9a88b14eadc765d'), 'places': ['Africa', 'Toronto', 'V.I.', 'earth', 'Earth', 'Europa', 'New York City', 'Murray Hill', 'Turing', 'California', 'Manchester', 'New Jersey', 'the S. S', 'Canada', 'coauthoring', 'Berkeley Heights', 'Cambridge', 'Asia', 'Princeton', 'New Providence', 'Leks']}\n",
      "{'_id': ObjectId('5fe6865bc9a88b14eadc765e'), 'places': ['Washington', 'Arizona', 'Turing', 'Berkeley', 'Munich']}\n",
      "{'_id': ObjectId('5fe68667c9a88b14eadc765f'), 'places': ['et cetera', 'ofay', 'Chernobyl', 'France', 'earth', 'Earth', 'China', 'Santa Claus', 'Pentecost', 'the Soviet Union', 'Pardot', 'Atacama', 'Fukushima', 'the New Testament', 'the Three Mile Island', 'Japan', 'US', 'India', 'Cambridge', 'Asia', 'Europe', 'West', 'Livermore', 'us', 'Deuch Runs', 'America']}\n",
      "{'_id': ObjectId('5fe68671c9a88b14eadc7660'), 'places': ['China', 'Hong Kong', 'Alver', 'India', 'Nipsy', 'Puti', 'Jupiter', 'M.O.', 'Pewaukee', 'Mars', 'San Francisco', 'US', 'Russia', 'Bitcoin', 'Singapore', 'South America', 'Kazarian']}\n",
      "{'_id': ObjectId('5fe6867ec9a88b14eadc7661'), 'places': ['Central Park', 'SIM City', 'Greece', 'Vienna', 'earth', 'Vegas', 'New York', 'Earth', 'the United States', 'Singapore', 'Washington', 'Belgium', 'Western', 'Pennsylvania', 'Maine', 'Oregon', 'kingdom', 'Philadelphia', 'the Galapagos Island', 'Boston', 'Canada', 'west', 'PhD', 'Galapagos', 'Pacific Rim', 'Bahamas', 'UMass', 'Amherst', 'east']}\n",
      "{'_id': ObjectId('5fe6868dc9a88b14eadc7662'), 'places': ['East Asia', 'Africa', 'the Library of Alexandria', 'Ujamaa', 'Greece', 'Mars', 'Manaus', 'Earth', 'China', 'Moderne', 'H2', 'South America', 'California', 'Zaccaro', 'the Gulf of Mexico', 'Boston', 'Texas', 'US', 'Egypt', 'Russia', 'India', 'Northern Europe', 'Moscow', 'Athens', 'Jupiter', 'North Italy', 'NewLink', 'Europe', 'Paris', 'Italy', 'Machu Picchu', 'us']}\n",
      "{'_id': ObjectId('5fe6869ac9a88b14eadc7663'), 'places': ['Arizona', 'V.I.', 'Las Vegas', 'A.I.', 'the United States', 'Singapore', 'East Coast', 'Ventura', 'China', 'Silicon Valley', 'L.A.', 'The West Coast', 'Boston', 'B.C.', 'US', 'Phoenix', 'Russia', 'India', 'Europe', 'us']}\n",
      "{'_id': ObjectId('5fe686a7c9a88b14eadc7664'), 'places': ['Mitic', 'halleluja.', 'Namiki', 'the Planet Earth', 'New Hampshire', 'Greece', 'Mars', 'France', 'earth', 'Earth', 'New York', 'the United States', 'China', 'the Kilimanjaro Mountain', 'Kenya', 'Averroes', 'Sea', 'Death Valley', 'Boston', 'hieroglyphics', 'India', 'US', 'Egypt', 'Bitcoin', 'Russia', 'Harasta', 'Nola', 'Amsterdam', 'Sterk', 'America']}\n",
      "{'_id': ObjectId('5fe686b3c9a88b14eadc7665'), 'places': ['England', 'Fanni', 'Reburn', 'Iturbi', 'SIM City', 'Marcovicci', 'Mars', 'Éminence', 'Newcastle', 'Pushtu', 'Turing', 'Tenzer', 'Norge']}\n",
      "{'_id': ObjectId('5fe686bfc9a88b14eadc7666'), 'places': ['Earth', 'Philadelphia', 'Alberta', 'New Jersey', 'Providence', 'South Africa', 'Europe', 'Rhode Island', 'SIM City', 'Turing', 'Philly', 'Bachchu', 'the Soviet Union', 'Reno', 'earth', 'Michigan', 'the United States', 'England']}\n"
     ]
    }
   ],
   "source": [
    "for arr in collection.find({},{\"places\":1}):\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(collection.find({\"guest\":\"Ben Goertzel\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bro\n",
      "dict_keys(['entis', 'keywords', 'summary', 'subtopics'])\n",
      "CPU times: user 13.3 s, sys: 4.06 s, total: 17.4 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"bro\")\n",
    "doc = nlp(text_fix(open(\"3Lex/#103|Ben_Goertzel|Artificial_General_Intelligence.txt\").read()))\n",
    "print(doc.user_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = doc.user_data[\"entis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = list(set([e[0] for e in ents if e[1]==\"LOC\" or e[1]==\"GPE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_person(name):\n",
    "    try:\n",
    "        result = wikipedia.search(name)[0]\n",
    "        if len(result.split(\" \"))>1 and name in result:\n",
    "            return(result, True)\n",
    "        else:\n",
    "            return (name, False)\n",
    "    except:\n",
    "        return(\"none\", False)\n",
    "def keep_ents(doc):\n",
    "    books, people, places = [], [], []\n",
    "    ents = [e for e in doc.user_data[\"entis\"] if e[0].replace(\".\",\"\").lower()!=\"phd\"]\n",
    "    \n",
    "    doc.user_data[\"places\"] = list(set([e[0] for e in ents if e[1]==\"LOC\" or e[1]==\"GPE\"]))\n",
    "    \n",
    "    #parallel processing to verify people and get full names from wikipedia\n",
    "    people = list(set([e[0] for e in ents if e[1]==\"PERSON\"]))\n",
    "    finalpeople = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 30) as executor:\n",
    "        result = [executor.submit(is_person, p) for p in people]\n",
    "    for future in concurrent.futures.as_completed(result):\n",
    "#         print(future.result())\n",
    "        if future.result()[1]==True:\n",
    "            finalpeople.append(future.result()[0])\n",
    "    doc.user_data[\"people\"] = finalpeople\n",
    "    \n",
    "    #parallel processing to verify books\n",
    "    books = list(set([e[0] for e in ents if e[1]==\"WORK_OF_ART\"]))\n",
    "    allbooks = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 30) as executor:\n",
    "        result = [executor.submit(is_book, book) for book in books]\n",
    "    for future in concurrent.futures.as_completed(result):\n",
    "        if future.result()[1]==True:\n",
    "            allbooks.append(future.result()[0])\n",
    "    doc.user_data[\"books\"] =allbooks\n",
    "    return doc\n",
    "doc = keep_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['So he had a very broad view of intelligence going beyond the human and into what i would call, open ended superintelligence that the solaris superintelligent ocean was intelligent in some ways more generally intelligent than people, but in a complex and confusing way so that human beings could never quite connect to it.',\n",
       "  'Naturally occurring intelligence then philip k. dick, who, ultimately my friend and philip k. dick is one of the things that brought me together with david hanson, my collaborator on robotics project.',\n",
       "  'And there was not that much science fiction to watch on tv at that stage that got me into reading the whole literature of science fiction, from the beginning of the previous century and until that time.',\n",
       "  'The show, sponsored by a master class sign up, a master class that looks to get a discount and to support this podcast.',\n",
       "  'Please consider supporting this podcast by going to jordan harbinger, that complex and signing up a master class'],\n",
       " ['Is it the human side of somehow persisting through all of the different systems we engineer, or is it or is i inspire you to create something that is greater than human, that is beyond human, that is almost non-human?',\n",
       "  'So what we perceive as random fluctuations and the quantum or some quantum level may actually be the thoughts of the micro, micro, micro, miniaturised super intelligences, because there is no way we can tell random from structured, but with an algorithmic information more complex than our brains.',\n",
       "  'And yeah, just ambition to create something much better than all the clearly limited and fundamentally defective humans i saw around me then as i got older and got more enmeshed in the human world and, got married, had children.',\n",
       "  'They just think i am sitting there wiggling my fingers to exercise and maybe or guarding the monitor on the desk that they have no idea that i am communicating with other people halfway around the world, let alone, creating complex algorithms running in rhyme on some computer server in st. petersburg or something like that.',\n",
       "  'The minds of the super, super, super intelligences, they are going to be packed into the interaction of elementary particles or quarks or the protons inside quarks or whatever it is.'],\n",
       " ['My my father’s father, victor garozzo, was a phd in psychology who had the unenviable job of giving psychotherapy to the japanese in internment camps in the us in war in world war two to counsel them.',\n",
       "  ', when i saw a robot on star trek, it was turning around in a circle, going around iraq, iran, because spock and kirk had tricked into a mechanical breakdown by presenting with a logical paradox.',\n",
       "  'My dad, he was programming fortran when i was 10 or 11 years old on like three thousand mainframes at rutgers university.',\n",
       "  'And before i was born, when my dad was an antioch college in the middle of the us, he led a protest movement called slum student league against mortality.',\n",
       "  'So, my three of my great grandparents emigrated to new york from lithuania and sort of border regions of poland, which are in and out of poland in around the time of world war one.'],\n",
       " ['So, yeah, it is easy, easy for people to lose track now of the fact that the futurist and singularity and advanced technology ideas that are now almost mainstream are on tv all the time.',\n",
       "  'N. should send people out to every little village in remotest africa, south america, and explain to everyone what technology was going to bring the next few decades and the choice that we had about how to use it and let everyone and the whole planet vote about whether we should develop, super',\n",
       "  'These are not that new, right, they are sort of new in the history of the human species, but these are all around in fairly mature form in the middle of the last century, were written about quite articulately by fairly mainstream people who are professors at top universities.',\n",
       "  'Human immortality and time travel are all the same things as every other, like science fiction loving kid, but i seem like if hofstetter was right, you just figure out the right programs that they are entitled.',\n",
       "  'So i think that book for the last robock, which i think i read when it first came out, i would have been 12 years old or something.'],\n",
       " ['So i think if you look at the evolution, humanity is shaped both by individual selection and what biologists would call group selection, like tribe level selection.',\n",
       "  'If we look at i know it is hard to look at humans in the aggregate, but do you think overall humans are good?',\n",
       "  'There are many particularities to the human values which whether they are good or evil, depends on here on your perspective or let us say i spent a lot of time in ethiopia, in addis ababa, where we have one of our eye development officers for my singularity project.',\n",
       "  'Tribal like group selection means humans in a way will do what will advocate for the persistence of the dna of their whole tribe or their social group.',\n",
       "  'He had a lot of inklings of modern cognitive science, which are very interesting, if you look in like the third part of the collection that is been titled the will to power and put through there, there is very deep analysis of thinking processes.'],\n",
       " ['I think assuming we make a mars colony, people go live there in a couple of decades, their supplies are going to come from earth.',\n",
       "  'I used to argue this with my great grandparents who were marxists, actually, because they believed in the withering away of the state, like they believe that, as you move from capitalism to socialism to communism, people would just become more social minded so that a state would be unnecessary and people would just give everyone would give everyone else what they needed.',\n",
       "  'You could have a human society that was dialed dramatically, further toward, self-awareness, their awareness, compassion and sharing, then our current society and of course, greater material abundance helps.',\n",
       "  'So there is a hunger for new types of governments, new types of leadership, new types of systems.',\n",
       "  'How to get there is quite a quite different question, because there are very powerful forces pushing people in different directions than the positive, joyous, compassionate existence.'],\n",
       " ['It was either sheinberg or peter viles on the private email discussion we had just about when we go with ajai artificial general intelligence and tawang want to do jei general artificial intelligence, because in chinese it goes in that order.',\n",
       "  'He is looking at complex self organising systems and looking at an intelligence system as being one that, revises and grows and improves itself in conjunction with it, with its environment, without necessarily there being one objective function it is trying to maximize, although over certain intervals of time, it may act as if it is optimizing a certain objective function.',\n",
       "  'I do not think that is critical to achieve ajai, but certainly you could see how the right quantum computing architecture could massively accelerate ajai similar to other types of nanotechnology.',\n",
       "  'You know, josh harbach, who you interviewed, is a close friend of mine, and he likes the term synthetic intelligence, which i like much better, but it has not actually caught on because, artificial is a bit off to me because, ah, the face is like a tool or something.',\n",
       "  'On the other hand, i do not think mars colonization or inventing amazing new genres of music is it is not one of the things that is most likely to make a critical difference in the evolution of human or non-human life in this part of the universe over the next decade.'],\n",
       " ['So yeah, the ajai conferences are sort of now the main concentration of people not obsessed with deep neural net deep reinforcement learning but still interested in energy and not not the only ones.',\n",
       "  'But that was a system that was supposed to be an ajai and basically by some sort of fancy like markov decision process learning it was supposed to learn everything just from the bits coming into it and learning to maximize its reward and become intelligent.',\n",
       "  'Now john cleary helped to work out which is the first open source machine learning to get to the predecessor for tensas flow and torture and all these things.',\n",
       "  'So you look at john andrius, the university of canterbury, with its purpose reinforcement learning murkoff system.',\n",
       "  'Probability theory based reinforcement learning protegé gi john cleary was trying to do much more ambitious probabilistic existence.'],\n",
       " ['So, to comment on the role of ajai in the research community, i would still if you look at europe’s, if you look at gdp, if you look at these, i clear, asia is still seen as the outcaste.',\n",
       "  'So, if you were going to take a theory based approach to ajai, what you would do is say, well, let us take what is called a exito, which is hodas x ray machine that can work on merely insanely much processing power rather than what is to stand for time and light.',\n",
       "  'You were way, way, way, way out of the industry, academic mainstream.',\n",
       "  'And in some sense, the triple a.i., the artificial intelligence, the main kind of generic artificial intelligence committee conference is too big.',\n",
       "  'I would say in these main machine learning, in these main artificial intelligence conferences amongst the researchers, i do not know if it is an accepted term yet.'],\n",
       " [\"The way i would want to implement ajai on a bunch of neurons in a vat that i could rewire arbitrarily is quite different than the way i would want to create ajai on, say, a modern server farm of cpu's and gps, which in turn may be quite different in the way i would want to implement ajai on, whatever quantum computer will have in 10 years.\",\n",
       "  'Some constraints, we have these processing power constraints and, we have space and time constraints on the program, we have energy utilization constraints and we have this particular class environment class of environments that we care about which maybe say, manipulating physical objects on the surface of the earth, communicating in human language.',\n",
       "  'Whatever our particular not annihilating humanity or whatever our particular requirements are going to be, if you formalize those requirements in some formal specification language, you should then be able to run automated programs specialized on exito, specialize it to the computing resource constraints and the particular environment and go.',\n",
       "  'And when you look at neural networks, that is one powerful class of learning algorithms, but it is also a class of learning algorithms that evolved to exploit the particulars of the human brain as a computational substrate.',\n",
       "  'And then it looks at all possible programs it could use to make a decision and it decides like which decision program would have let it make the best decisions according to its reward function over its history and the uses that decision program to take to make the next decision.'],\n",
       " ['In the past, human brain has substantially different subsystems for these different types of memory and substantially differently tuned learning, differently tuned modes of long term potentiation to do with the types of neurons and neurotransmitters and the different parts of the brain corresponding to these different types of knowledge and these different types of memory and learning in the human brain.',\n",
       "  'So there the dnc would be an example of folks from the deep neural network trying to take a step in the cognitive architecture direction of having two neural modules that correspond roughly to two different parts of the human brain that deal with different kinds of memory and learning.',\n",
       "  'So if you start to look at, how would you realize some specialized or constrained version of universal general intelligence in a system that has, limited memory and limited speed of processing, but whose general intelligence will be biased toward controlling a solid object agent, which is mobile in a solid object world for manipulating solid objects and communicating via language with other similar agents in that same world.',\n",
       "  'But on the other hand, it is super, super, super crude from the cognitive architecture of just as what john learned and so did with neural nets was super crude from a learning point of view, because the learning was like after the side not affecting the core representations.',\n",
       "  'Ten years ago, before this whole commercial mining explosion was on the one hand, you have these cognitive architecture guys who were working closely with psychologists and cognitive scientists who had thought a lot about how the different parts of a human like mine should work together.'],\n",
       " ['You have a link between three nodes and in fact open space would probably be called a meta graph because you can have links pointing to links where you could have links between the whole sub.',\n",
       "  'On the one hand, as a software framework could be used to implement a variety of different ai architectures and algorithms, but in practice there has been a group of developers which i have been leading together with linus webster, snow guys viler and a few others, which have been using the open cut platform and infrastructure to implement certain ideas about how to make an agi.',\n",
       "  'So the core component of open cog is a software platform is what we call the admin space, which is a weighted labeled hydrograph 80 oem atom space, atom space, not atom like adam and eve, although that would be cool.',\n",
       "  'And of course, like with all things, you can reduce that to a hypergraphia and hyperglycemic or just hydrograph to a graph and you could use a graph to an adjacency matrix.',\n",
       "  'A hyper graph is like a graph, but links can go between more than two nodes.'],\n",
       " ['But the current systems, uh, not being look, for example, if you wanted to make a biologically realistic hardware neural network, like taking making a. circuit in hardware that emulated like the hodgkin huxley equation or the isakov education, like equate differential equations for a biologically realistic neuron and putting that in hardware on the chip, that would seem that would make more feasible to make a large scale, truly, biologically realistic neural network.',\n",
       "  'But i think in the next, let us say, three to five years, we are going to see new chips where like a graph is put on the chip and, the back and forth between multiple processes acting cindy and mindy on that graph is going to be fast.',\n",
       "  'With an ajai hat on, i am just more interested in these hydrograph knowledge, representation based architectures, which would benefit more from various types of grauwe processors, because the main processing bottleneck is reading, writing to ram.',\n",
       "  'So if you want to do something like that, having neuromorphic hardware that really let you simulate like a realistic model of the neuron would be, it would be amazing.',\n",
       "  'We were looking at how do you make a biologically realistic simulation of seven different parts of the brain cooperating with each other using like realistic, nonlinear dynamical models of neurons?'],\n",
       " ['You can also have procedure like nodes and links as in, say, combinatorial logic or lambda calculus representing programs so you can have nodes and links representing many different types of semantics, which means you could make a horrible, ugly mess or you can make a system where these different types of knowledge are interpenetrate and synergized with each other beautifully.',\n",
       "  'Abstract, declarative knowledge and sensory knowledge and movement, knowledge and procedural knowledge and episodic knowledge, finding the right level of representation, where all these types of knowledge are stored in a sort of universal and inter convertible, yet practically manipulable way.',\n",
       "  'It is choosing what type system you want to put on the nodes and links in the hydrograph, what types of nodes and links you want.',\n",
       "  'Ropen kog, you can update the weights, and that certainly happens a lot, but adding new nodes, adding new links removed, removing nodes and links is an equally critical part of the systems operations.',\n",
       "  'You want the logic engine when it gets stock to be able to share its intermediate state with the neural net and with the evolutionary learning algorithm so that they can help each other out of bottlenecks and help each other solve combinatorial explosions by intervening inside each other’s cognitive processes.'],\n",
       " ['We tried to find elegant ways of sort of hierarchically breaking down complex logic expression into nodes and links so that if you have, say, different nodes representing, then i leks interview or whatever, the logic, relations between those things are compact in the northern link representation so that when you have a neural net acting on the same nodes and links, the neural net and the logic engine can sort of interoperate with each other and also interpretable by humans.',\n",
       "  'But term logic breaks down basic logic into basically simple links between nodes like inheritance link between no nowaday and not so in term logic.',\n",
       "  'Well, if you want to reason on how to improve that procedure, you need to map that procedure into logic using howard isomorphic and certain in the logic, the logic engine can reason about how to improve that procedure and then map that back into the procedural representation that is efficient for execution.',\n",
       "  'But by breaking down logic into term logic, you get a nice way of breaking logic down into into nodes and links',\n",
       "  'Logic relations, including basic sort of proposition logic relations, as aristotelian term logic deals with and then quantifier logic relations also, how do you break those down elegantly into a graph?'],\n",
       " ['So like when you if you have a rule of grammar and you are not sure if it is the correct rule of grammar or not, you can generate a bunch of sentences using that rule of grammar and a bunch of sentences violating that rule of grammar.',\n",
       "  'That just had not been one of our design thoughts when we opened congreso between morning, really fast dependent type checking and wanting much more efficient in their operation between the computation graphs of deep neural net frameworks and open cogs, hyper graph and adding on top of that one to more effectively run an open hyper graph distributed across the room in 10000 machines, which is we are doing dozens of machines now, but it is just not we did not architect it with that sort of modern scalability in mind.',\n",
       "  'So what we are actually doing is we are using a symbolic grammar learning algorithm, but we are using the terms from a neural network as a sentence probability oracle.',\n",
       "  'So in that way, you can use the neural model as a probability oracle to guide symbolic grammar learning process.',\n",
       "  'So you can that is one way you use a structured learning algorithm, which is symbolic, and then you use the deep neural net as an oracle to guide the structure.'],\n",
       " [', he had the idea of a society of minds like you should achieve an ai, not by writing one algorithm or one program, but you should put a bunch of different ideas out there and the different eyes will interact with each other, each playing their own role and then the totality of the society of what would be the thing that displayed the human level intelligence.',\n",
       "  'Ok, let us make it decentralized agent system where a bunch of different eyes, wrapped up and say different docker containers or aleksi containers, different eyes.',\n",
       "  'And then these are all cooperating, coordinating together, sort of like in the brain, ok, the brain as a whole is the general intelligence, but some parts of the cortex, you could say, have a fair bit of general intelligence on their own where, say, parts of the cerebellum or limbic system have very little intelligence on their own.',\n",
       "  'It was, take 25 different neural modules architected in different ways, maybe resembling different parts of the brain, like a basal ganglia model, cerebellum or thallium.',\n",
       "  'Than i do i think you could have a mind that was as disorganized as a human society, but i think a human like mine has a bit more central control than that, actually.'],\n",
       " ['So ocean protocol is basically block chain based big data and names that make making it efficient for four different a.i. processes or statistical processes or whatever to share large data sets or one process can send a clone of itself to work on the other guy’s data set and send results back and so forth.',\n",
       "  'Will just be on the block chain in a way that enforces centralized control and government hedge money rather than otherwise, like the rmv will probably the first global the first currency on the block in the airable, maybe next year already.',\n",
       "  'But it is quite challenging because to build this whole decentralized block chain based infrastructure, your competitors are like google, microsoft, alibaba and amazon, which have so much money to put behind their centralized infrastructures, plus the solving simpler algorithmic problems because making it centralized in some ways is easier.',\n",
       "  'But then that open cog will interface with other ais doing deep neural nets or custom biology data analysis or whatever they are doing in singularity, which is a looser integration of different, some of which may be maybe they are their own networks.',\n",
       "  'That is the project of mcconaghy, who developed big chain db, which is a block chain based database.'],\n",
       " ['And he thought the way to do that was to make a machine that can, look, people are the eye, face to face, look at people and make people love the machine and',\n",
       "  'Would be a platform much like the pepper robot is a platform from something, should be a platform with a set of nicely designed apis that anyone can use to experiment with their different ai algorithms on that platform.',\n",
       "  'You can use your decentralized protocol, so say some developers from iran, and there is brilliant ai guys in university of isfahan in tehran, they can put this stuff on singularity in that protocol.',\n",
       "  'In the same way, we got to put out there in a decentralized vein and take that out there in a decentralized vein now so that the most advanced ai in the world is fundamentally decentralized.',\n",
       "  'Also, i invited him to hong kong to give a talk at hong kong.'],\n",
       " ['So it seemed to me making these like beautiful, loving robots to be rolled out for beneficial applications would be the perfect way to roll out early stage ajai systems so they can learn from people and not just learn, in fact, from the ultimate learning, human values and ethics from people while being there.',\n",
       "  'And the other way is you make like a simple version of the whole system and put something in the place of every part the whole system will need so that you have a whole system that does something',\n",
       "  'I algorithms out there in front of a whole lot of different people in an emotionally compelling way, and part of my thought was really kind of abstract connected to high ethics.',\n",
       "  'The best way to fire stages we create toward benevolence would be to infuse them with love and compassion the way that we do our own children.',\n",
       "  'There is computer vision to recognize people’s faces, recognize when someone comes in the room and leaves to recognize whether two people are together or not.'],\n",
       " ['So if your ethics allows you to use machine learning in such a blatantly destructive way, why would you ethics not allow you to use machine learning to make a loveable theatrical robot that draws some foolish people into its theatrical illusion?',\n",
       "  'All these stupid people out there think this is an ajai, but it is not an ajai, but they are checking people that this very cool robot is a najai.',\n",
       "  'Ironic because john mccain is working for facebook, which is using machine learning to program the brains of the people in the world toward vapid consumerism and political extremism.',\n",
       "  'Even if i were not directly involved with it once i dug a little deeper into david and the robot and the intentions behind it, i think i would have stopped being pissed off with folks like young lachian have remained pissed off after their initial while and their initial thing.',\n",
       "  'And then they criticize people like you who sit back and do not say anything about like basically allow the imagination of the world that allow the world to continue being captivated.'],\n",
       " ['After the conversation, we brought the people in the back room to see stephane, who was controlling the philip k. dick robot.',\n",
       "  'Let us be more open about how this thing is working, and i did have some influence in nudging hanson robotics to be more open about how software was working.',\n",
       "  'And david also does not agree with everything david has said or done about fourteen point.',\n",
       "  'We just said, here, have a conversation with phil dick, we are going to film you.',\n",
       "  'And they had a great conversation with phil kadek operated by my friends to find the guy.'],\n",
       " [\"And we are fooling people 100 percent toward a good and we are playing on people’s sense of empathy and compassion so that we can give them a good user experience with help for robots and so that we can fill the ai's mind with love and compassion.\",\n",
       "  'And that is, that is, that is part of that is going to be part of our journey and creating intelligence systems more and more like as sofia starts out with the walking skeleton, as you add more and more intelligence, we are going to have to deal with this kind of idea about sophia.',\n",
       "  'So when she whether she is really learning or thinking or just appears to be over half hour, i could tell with over like three or four minutes of interaction, i even having three systems that is already sufficiently complex where you can not really tell right away.',\n",
       "  'It reacts quite in a fluid and flexible way, but we immediately ascribe the kind of intelligence we media ascribe to them.',\n",
       "  'And the the whole media industry is based on fooling people the way they want to be fooled.'],\n",
       " ['But so this biotech space that we built for these more commercial and longevity data analysis purposes, we are repurposing and covid data into the same biome space and playing around with, like graffin beddings from that graph into neural nets for bioinformatics.',\n",
       "  'But what should be the case is like every covid clinical trial should be putting data online somewhere like suitably encrypted to protect patient privacy so that anyone with the a.i. algorithms should be able to help analyze it and any biologist should be able to analyze it by hand to understand what they can write instead.',\n",
       "  'So we have been talking a lot about, robots can help with elder care robots going out with kids, davidson, lot of things with autism therapy and robots, robots before in the car over there, having a robot that can be a nursing assistant in various senses can be quite valuable.',\n",
       "  'And it seems we are able to discover things that people were not seeing otherwise, because the thing in this case is for each combination of antivirals, you may have only a few patients who have tried that combination and those few patients may have their particular characteristics like this combination of three was tried only on people aged 80 or over.',\n",
       "  'Machine reasoning is interesting because it can help with transfer learning when you have not that many different cases to study and qualitative differences between different strains of a virus or people of different ages who may have koven.'],\n",
       " ['This, i think, is a greater risk to humanity from a.i. than rogue ages, turning the universe into paper clips or computer ternium, because what you have here is mostly good hearted and nice people who are sucked into a mode of organization of large corporations, which has evolved just for no individual’s fault, because that is the way society has evolved.',\n",
       "  'As well as just working on cool projects, you are coding stuff that gets used by like billions and billions of people and you think if i improve this feature that is making billions of people’s lives easier.',\n",
       "  'I think clearly the whole global medical system, the global health system and the global political and social economic system are incredibly unethical and unequal and badly designed.',\n",
       "  'I think if you spend much time in sub-saharan africa, you can see there is a lot of pain and suffering happening all the time, like you walk through the streets of any large.',\n",
       "  'This is the first time like a race against the clock and try to use the eye to figure out stuff that like if we take two months longer to solve the problem, some more people will die because we do not know what combination of antivirals to give.'],\n",
       " ['And so i think if you believe that ajai is going to emerge sort of incrementally and is doing practical stuff in the world, like controlling humanoid robots or driving cars or diagnosing diseases or operating killer drones or spying on people and repairing under the government, then what kind of organization creates more and more advanced?',\n",
       "  'The issue with a small elite group that knows what is best is even if it starts out as truly benevolent and doing good things in accordance with its initial good intentions, you find that you need more resources, you need a bigger organization.',\n",
       "  'But so far, the track record of elite groups who know what is better for all of humanity is much worse than the track record of the whole teeming, democratic, participatory mass of humanity.',\n",
       "  'So, yeah, most people on the planet setting aside a few genuine psychopaths and sociopaths, most people on the planet have a heavy dose of benevolence and wanting to do good and also having capability to convince themselves whatever they feel like doing or',\n",
       "  'I think if anyone can post something like this off, whether using the specific technologies i have mentioned or something else, then i think we have a higher odds of moving toward a beneficial technological singularity rather than one in which the first super ajai is indifferent to humans and just considers us an inefficient use of molecules.'],\n",
       " ['So i think in a practical sense, much of the meaning i see in human life is to create something better than humans and go beyond human life.',\n",
       "  'How flexibly can that more fair society and technology change, so if we are given that dial and we are given a society in which say we do not have to, we do not have to work for a living and in which there is an ambient, decentralized, benevolent network that will warn us when we are about to hurt ourselves, if we are in a different context, can we consistently with being genuinely and fully human, can we consistently get into a state of consciousness where we just want to keep the pain dial',\n",
       "  'And just because people are able to derive meaning and value from death does not mean they would not derive even better meaning and value from ongoing life without death, which is a very definite',\n",
       "  'I think human body is great and by no means any of us maximize the potential for joy, growth and choice in our human bodies.',\n",
       "  'So i think it might or might not be true that humans need a certain element of suffering to be satisfied, humans consistent with the human physiology.'],\n",
       " ['So, for example, the extracellular matrix, which is the bunch of proteins in between the cells in your body, they get stiffer and stiffer as you get older and the extracellular matrix transmits information both electrically, mechanically and to some extent bio photonics.',\n",
       "  'Ajai prologis, statistical research aimed at solving biology, both molecular biology and human biology based on this massive, massive data set.',\n",
       "  'I got two more quick questions for you, one, i know a lot of people are going to ask me, you are joe rogan podcast wearing that same amazing hat.',\n",
       "  'So there is all this transmission through the parts of the body, but the stiffer the extracellular matrix gets, the less the transmission happens, which makes your body get worse, coordinate between the different organs as you get older.',\n",
       "  'Trillions are being given the large banks and insurance companies anyway, like could the world put 10 trillion dollars into making a massive holistic buy and bio simulation and experimental biology infrastructure?'],\n",
       " ['I know you interviewed elon musk and he understands a lot of what is going on, but he is much more paranoid than i am because elon gets that guy is going to be way, way smarter than people and he gets an ajai, does not necessarily have to give a shit about people because we are a very elementary mode of organization of murder compared to many, many ages.',\n",
       "  'The thing is, once you are a super, super ajai, like once subjective second to a human being, like a million subjective years to that super ejiro.',\n",
       "  'But i do not think he has a clear vision of how infusing early stages with compassion and human warmth can lead to an ajai that loves and helps people rather than viewing us as, a historical artifact and a waste of mass energy.',\n",
       "  'The thing is to get to that future where we can explore different varieties of joy, different variations of human experience and values and transhuman experiences and values.',\n",
       "  'Make another version which fuses its mind with superhuman ajai and then will become massively transhuman and whether it will send some messages back to the human, me or not, it will be interesting to find out.']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.user_data[\"subtopics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(collection.insertOne(doc.user_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in doc.user_data[\"subtopics\"]:\n",
    "    print(\" \".join(topic[:2]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_transcripts(html_location, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %run -i 'myimports.py'\n",
    "#convert file of podcasts to list of docs and docbin and write to disk\n",
    "# %run -i 'nlp_implementation.py'\n",
    "podcast_name = \"3Lex\"\n",
    "# print(contractions.fix(\"hey what's up\"))\n",
    "\n",
    "os.listdir(podcast_name)\n",
    "onlyfiles = folder_to_filelist(podcast_name) #get list of all files in foldername\n",
    "# doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "docs = [] #list of docs \n",
    "print(\"starting\")\n",
    "for doc, name in nlp.pipe(onlyfiles, as_tuples=True): #piping all collection of docs to make doclist and docbin\n",
    "    #each doc contains hostname, guest, title, entities mentioned, and summary\n",
    "    name = re.split(\"[\\|]\",name)\n",
    "    name=name[1:] if name[0][1:].isdigit() else name # store name of guest and topic, add to doc user data\n",
    "    \n",
    "    doc.user_data[\"host\"] = podcast_host\n",
    "    doc.user_data[\"guest\"]= str(name[0]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"title\"]= str(name[1][:-4]).replace(\"_\",\" \")\n",
    "    \n",
    "    docs.append(doc)\n",
    "    collection.insert_one(doc.user_data)\n",
    "#     doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "# with open(\"cucumber/Lexnumbered.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "#     f.write(doc_bin.to_bytes())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in collection.find({\"host\":\"Lex Fridman\"}):\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))\n",
    "# with open(\"cucumber/first_doc.bin\", \"rb\") as w: #loading in\n",
    "#     new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "# docs1 += list(new_docs1.get_docs(nlp.vocab))\n",
    "for doc in docs1:\n",
    "    print(doc.user_data[\"title\"], doc.user_data[\"guest\"])\n",
    "# print(docs1[5].user_data[\"books\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each category of ents, generate csv files for ents corresponding to people\n",
    "\n",
    "att_to_csv(docs1, \"people\") \n",
    "# att_to_csv(docs1, \"places\")\n",
    "# att_to_csv(docs1, \"books\")\n",
    "# print(sorted(set([item for sublist in docs1 for item in sublist.user_data[\"people\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate megagraph for book ents\n",
    "edges = pd.read_csv(\"booksEdges.csv\", sep=',').drop(\"id\", axis=1).values.tolist()\n",
    "nodes = pd.read_csv('booksNodes.csv',sep=',').set_index(\"id\").to_dict(\"index\")\n",
    "\n",
    "source = [nodes.get(e[0])[\"name\"] for e in edges]\n",
    "target = [nodes.get(e[1])[\"name\"] for e in edges]\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':np.ones(len(source))})\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()\n",
    "\n",
    "#following are other example graphviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"Bible\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"Matthew Johnson\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['source']==\"Matthew Johnson\" )| (kg_df['source']==\"Ryan Hall\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['target']==\"Crime and Punishment\" )| (kg_df['target']==\"Atlas Shrugged\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was a very astute analyst of, you know, how the human mind constructs the illusion of itself, how it constructs the illusion of free will, how it constructs values like like good and evil out of its own, you know, desire to maintain and advance its own organism\n",
      "So i think that book for the last robock, which i think i read when it first came out, i would have been 12 years old or something\n",
      "So i think if you look at the evolution, humanity is shaped both by individual selection and what biologists would call group selection, like tribe level selection\n",
      "should send people out to every little village in remotest africa, south america, and explain to everyone what technology was going to bring the next few decades and the choice that we had about how to use it and let everyone and the whole planet vote about whether we should develop, you know, If we look at i know it is hard to look at humans in the aggregate, but do you think overall humans are good?\n",
      "Summarize Text: \n",
      " So i think if you look at the evolution, humanity is shaped both by individual selection and what biologists would call group selection, like tribe level selection\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "#     print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# let's begin\n",
    "generate_summary( \"msft.txt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /ilab/users/hs884/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
