{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 1.98 s, total: 17.2 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#import in all libraries and methods\n",
    "%run -i 'myimports.py'\n",
    "%run -i 'datagathering.py'\n",
    "%run -i 'nlp_processing.py' #imports packages and methods\n",
    "import pymongo\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "podcast_host = \"Lex Fridman\"\n",
    "html_location = \"Archive/happyscribe_Lex_Fridman_html.txt\"\n",
    "# update_transcripts(html_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 1min 32s, sys: 34.5 s, total: 2min 7s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#convert file of podcasts to list of docs and docbin and write to disk\n",
    "\n",
    "# %run -i 'nlp_implementation.py'\n",
    "podcast_name = \"Archive/both\"\n",
    "# os.listdir(podcast_name)\n",
    "onlyfiles = folder_to_filelist(podcast_name) #get list of all files in foldername\n",
    "\n",
    "doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "docs = [] #list of docs \n",
    "print(\"starting\")\n",
    "for doc, name in nlp.pipe(onlyfiles, as_tuples=True): #piping all collection of docs to make doclist and docbin\n",
    "    #each doc contians hostname, guest, title, entities mentioned, and summary\n",
    "    name = re.split(\"[\\|]\",name)\n",
    "    name=name[1:] if name[0][1:].isdigit() else name # store name of guest and topic, add to doc user data\n",
    "    \n",
    "    doc.user_data[\"host\"] = podcast_host\n",
    "    doc.user_data[\"guest\"]= str(name[0]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"title\"]= str(name[1][:-4]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"entis\"] = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    doc = keep_ents(doc) #extract and save all entities by filtering entity extraction\n",
    "    docs.append(doc)\n",
    "    doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "with open(\"cucumber/Lex.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "    f.write(doc_bin.to_bytes())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Trek', 'Mind and Cosmos', 'Oculus Rift']\n",
      "['Thinking Fast and Slow', 'Manâ€™s Search for Meaning']\n",
      "[]\n",
      "['Space Odyssey']\n",
      "['The Art of Computer Programming', 'Bible']\n",
      "[]\n",
      "[]\n",
      "['Love in Math']\n",
      "['Ph.D.', 'Love and Math', 'The Bridge Over the Bridge']\n",
      "[]\n",
      "[]\n",
      "['Ph.D.', 'Guitar Zero', 'Deep Learning']\n",
      "['How Life Imitates Chess']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Ph.D.']\n",
      "[]\n",
      "[]\n",
      "CPU times: user 1.41 s, sys: 63.4 ms, total: 1.47 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))\n",
    "# with open(\"cucumber/first_doc.bin\", \"rb\") as w: #loading in\n",
    "#     new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "# docs1 += list(new_docs1.get_docs(nlp.vocab))\n",
    "for doc in docs:\n",
    "    print(doc.user_data[\"books\"])\n",
    "# print(docs1[5].user_data[\"books\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each category of ents, generate csv files for ents corresponding to people\n",
    "\n",
    "att_to_csv(docs1, \"people\") \n",
    "# att_to_csv(docs1, \"places\")\n",
    "# att_to_csv(docs1, \"books\")\n",
    "# print(sorted(set([item for sublist in docs1 for item in sublist.user_data[\"people\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate megagraph for book ents\n",
    "edges = pd.read_csv(\"booksEdges.csv\", sep=',').drop(\"id\", axis=1).values.tolist()\n",
    "nodes = pd.read_csv('booksNodes.csv',sep=',').set_index(\"id\").to_dict(\"index\")\n",
    "\n",
    "source = [nodes.get(e[0])[\"name\"] for e in edges]\n",
    "target = [nodes.get(e[1])[\"name\"] for e in edges]\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':np.ones(len(source))})\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()\n",
    "\n",
    "#following are other example graphviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"Bible\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"Matthew Johnson\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['source']==\"Matthew Johnson\" )| (kg_df['source']==\"Ryan Hall\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['target']==\"Crime and Punishment\" )| (kg_df['target']==\"Atlas Shrugged\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary(\"Yaron Brook\", sentences=3)\n",
    "wikipedia.summary(\"George Hotz\", sentences=3)\n",
    "wikipedia.summary(\"Atlas Shrugged\", sentences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
