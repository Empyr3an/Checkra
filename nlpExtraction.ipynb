{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "CPU times: user 15.7 s, sys: 1.48 s, total: 17.2 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "#import in all libraries and methods\n",
    "%run -i 'myimports.py'\n",
    "%run -i 'datagathering.py'\n",
    "%run -i 'nlp_processing.py' #imports packages and methods\n",
    "%run -i 'timestamp_generation.py'\n",
    "\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "podcast_host = \"Lex Fridman\"\n",
    "html_location = \"Archive/happyscribe_Lex_Fridman_html.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc = nlp(text_fix(open(\"3Lex/#103|Ben_Goertzel|Artificial_General_Intelligence.txt\").read()))\n",
    "print(doc.user_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_transcripts(html_location, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [11:23, 16.68s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python38/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, kwargs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36msubtopics\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mone_topic_confi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_confidences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_completion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#generate initial topics + confidences, then smooth by filling in empty values and averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(text, doc_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mbow_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_pod\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#dict for how many times each word appears\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#lda topic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "podcast_name=\"3Lex\"\n",
    "# docs, doc_bin = process_folder_to_docs(\"5Lex\", podcast_host)\n",
    "onlyfiles = folder_to_filelist(podcast_name) #get list (complete text, filename)\n",
    "doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "docs = [] #list of docs \n",
    "\n",
    "for doc, name in tqdm(nlp.pipe(onlyfiles, as_tuples=True)): #piping all collection of docs to make doclist and docbin\n",
    "    #each doc contains hostname, guest, title, entities mentioned, and summary\n",
    "    name = re.split(\"[\\|]\",name)\n",
    "    name=name[1:] if name[0][1:].isdigit() else name # store name of guest and topic, add to doc user data\n",
    "    \n",
    "    doc.user_data[\"host\"] = podcast_host\n",
    "    doc.user_data[\"guest\"]= str(name[0]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"title\"]= str(name[1][:-4]).replace(\"_\",\" \")\n",
    "    \n",
    "    docs.append(doc)\n",
    "    doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "# with open(\"cucumber/Lexnumbered.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "#     f.write(doc_bin.to_bytes())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Ben Goertzel Artificial General Intelligence\n",
      "1 Steven Pressfield The War of Art\n",
      "2 Alexander Fridman My Dad, the Plasma Physicist\n",
      "3 Robert Langer Edison of Medicine\n",
      "4 Joscha Bach Artificial Consciousness and the Nature of Reality\n",
      "5 Yaron Brook Ayn Rand and the Philosophy of Objectivism\n",
      "6 John Clarke The Art of Fighting and the Pursuit of Excellence\n",
      "7 David Patterson Computer Architecture and Data Storage\n",
      "8 Matt Botvinick Neuroscience, Psychology, and AI at DeepMind\n",
      "9 Matthew Johnson Psychedelics\n",
      "10 Andrew Huberman Neuroscience of Optimal Performance\n",
      "11 Scott Aaronson Quantum Computing\n",
      "12 Peter Singer Suffering in Humans, Animals, and AI\n",
      "13 Sergey Levine Robotics and Machine Learning\n",
      "14 Brian Kernighan UNIX, C, AWK, AMPL, and Go Programming\n",
      "15 Jitendra Malik Computer Vision\n",
      "16 Ian Hutchinson Nuclear Fusion, Plasma Physics, and Religion\n",
      "17 Andrew Ng Deep Learning, Education, and Real-World AI\n",
      "18 Lisa Feldman Barrett Love, Evolution, and the Human Brain\n",
      "19 Manolis Kellis Human Genome and Evolutionary Dynamics\n",
      "20 Erik Brynjolfsson Economics of AI, Social Networks, and Technology\n",
      "21 Manolis Kellis Meaning of Life, the Universe, and Everything\n",
      "22 Dileep George Brain-Inspired AI\n",
      "23 Michael Littman Reinforcement Learning and the Future of AI\n",
      "24 Richard Karp Algorithms and Computational Complexity\n",
      "25 Michael I. Jordan Machine Learning, Recommender Systems, and the Future of AI\n",
      "26 Russ Tedrake Underactuated Robotics, Control, Dynamics and Touch\n",
      "27 David Eagleman Neuroplasticity and the Livewired Brain\n",
      "28 John Hopfield Physics View of the Mind and Neurobiology\n",
      "29 Sheldon Solomon Death and Meaning\n",
      "30 Eugenia Kuyda Friendship with an AI Companion\n",
      "31 Marcus Hutter Universal Artificial Intelligence, AIXI, and AGI\n",
      "32 Sara Seager Search for Planets and Life Outside Our Solar System\n",
      "33 David Fravor UFOs, Aliens, Fighter Jets, and Aerospace Engineering\n",
      "34 Manolis Kellis Biology of Disease\n",
      "35 Nick Bostrom Simulation and Superintelligence\n",
      "36 Alex Garland Ex Machina, Devs, Annihilation, and the Poetry of Science\n",
      "37 Ann Druyan Cosmos, Carl Sagan, Voyager, and the Beauty of Science\n",
      "38 Vitalik Buterin Ethereum, Cryptocurrency, and the Future of Money\n",
      "39 Anca Dragan Human-Robot Interaction and Reward Engineering\n",
      "40 William MacAskill Effective Altruism\n"
     ]
    }
   ],
   "source": [
    "for ind, doc in enumerate(docs):\n",
    "    print(ind, doc.user_data[\"guest\"], doc.user_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#140|Lisa_Feldman_Barrett|Love,_Evolution,_and_the_Human_Brain.txt\n",
      "#144|Michael_Littman|Reinforcement_Learning_and_the_Future_of_AI.txt\n",
      "#121|Eugenia_Kuyda|Friendship_with_an_AI_Companion.txt\n",
      "#133|Manolis_Kellis|Biology_of_Disease.txt\n",
      "#117|Sheldon_Solomon|Death_and_Meaning.txt\n",
      "#115|Dileep_George|Brain-Inspired_AI.txt\n",
      "#116|Sara_Seager|Search_for_Planets_and_Life_Outside_Our_Solar_System.txt\n",
      "#77|Alex_Garland|Ex_Machina,_Devs,_Annihilation,_and_the_Poetry_of_Science.txt\n",
      "#80|Vitalik_Buterin|Ethereum,_Cryptocurrency,_and_the_Future_of_Money.txt\n",
      "#78|Ann_Druyan|Cosmos,_Carl_Sagan,_Voyager,_and_the_Beauty_of_Science.txt\n",
      "#104|David_Patterson|Computer_Architecture_and_Data_Storage.txt\n",
      "#138|Yaron_Brook|Ayn_Rand_and_the_Philosophy_of_Objectivism.txt\n",
      "#101|Joscha_Bach|Artificial_Consciousness_and_the_Nature_of_Reality.txt\n",
      "#100|Alexander_Fridman|My_Dad,_the_Plasma_Physicist.txt\n",
      "#122|David_Fravor|UFOs,_Aliens,_Fighter_Jets,_and_Aerospace_Engineering.txt\n",
      "#119|David_Eagleman|Neuroplasticity_and_the_Livewired_Brain.txt\n",
      "#84|William_MacAskill|Effective_Altruism.txt\n",
      "#83|Nick_Bostrom|Simulation_and_Superintelligence.txt\n",
      "#81|Anca_Dragan|Human-Robot_Interaction_and_Reward_Engineering.txt\n",
      "#102|Steven_Pressfield|The_War_of_Art.txt\n",
      "#103|Ben_Goertzel|Artificial_General_Intelligence.txt\n",
      "#113|Manolis_Kellis|Human_Genome_and_Evolutionary_Dynamics.txt\n",
      "#112|Ian_Hutchinson|Nuclear_Fusion,_Plasma_Physics,_and_Religion.txt\n",
      "#111|Richard_Karp|Algorithms_and_Computational_Complexity.txt\n",
      "#145|Matthew_Johnson|Psychedelics.txt\n",
      "#108|Sergey_Levine|Robotics_and_Machine_Learning.txt\n",
      "#139|Andrew_Huberman|Neuroscience_of_Optimal_Performance.txt\n",
      "#142|Manolis_Kellis|Meaning_of_Life,_the_Universe,_and_Everything.txt\n",
      "#143|John_Clarke|The_Art_of_Fighting_and_the_Pursuit_of_Excellence.txt\n",
      "#141|Erik_Brynjolfsson|Economics_of_AI,_Social_Networks,_and_Technology.txt\n",
      "#76|John_Hopfield|Physics_View_of_the_Mind_and_Neurobiology.txt\n",
      "#75|Marcus_Hutter|Universal_Artificial_Intelligence,_AIXI,_and_AGI.txt\n",
      "#74|Michael_I._Jordan|Machine_Learning,_Recommender_Systems,_and_the_Future_of_AI.txt\n",
      "#114|Russ_Tedrake|Underactuated_Robotics,_Control,_Dynamics_and_Touch.txt\n",
      "#110|Jitendra_Malik|Computer_Vision.txt\n",
      "#109|Brian_Kernighan|UNIX,_C,_AWK,_AMPL,_and_Go_Programming.txt\n",
      "#73|Andrew_Ng|Deep_Learning,_Education,_and_Real-World_AI.txt\n",
      "#107|Peter_Singer|Suffering_in_Humans,_Animals,_and_AI.txt\n",
      "#72|Scott_Aaronson|Quantum_Computing.txt\n",
      "#106|Matt_Botvinick|Neuroscience,_Psychology,_and_AI_at_DeepMind.txt\n",
      "#105|Robert_Langer|Edison_of_Medicine.txt\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "count = 0\n",
    "not_processed = []\n",
    "for ind, file in enumerate(os.listdir(\"5Lex\")):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file2 = file.replace(\"_\", \" \").replace(\"|\", \" \").replace(\".txt\", \" \")\n",
    "    #     print(ind, file) \n",
    "        processed = False\n",
    "\n",
    "\n",
    "        for doc in docs:\n",
    "            if doc.user_data[\"guest\"] in file2 and doc.user_data[\"title\"] in file2:\n",
    "    #             os.rename(\"5Lex/\"+file, \"7Lex/\"+file)\n",
    "                \n",
    "                processed = True\n",
    "                print(file)\n",
    "                count+=1\n",
    "                break\n",
    "        if processed:\n",
    "            os.remove(\"5Lex/\"+file)\n",
    "#             not_processed.append(file)\n",
    "print(len(not_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"5Lex\"):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"guest\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(docs[1].user_data[\"guest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 0 ns, total: 20.2 ms\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def att_to_csv(docs, atts):\n",
    "    all_atts= sorted(set([item for doc in docs for item in doc.user_data[atts]])) #all atts\n",
    "    all_attributes=dict([(x, [x]) for x in all_atts]) #dictionary of books to become key is base book, value are similar titles\n",
    "    i = 0\n",
    "#     similar_words implement dictionary to store similar words that were deleted\n",
    "    while i<len(all_attributes)-1: #remove similar or subwords\n",
    "        str1, str2 = all_atts[i], all_atts[i+1]\n",
    "        if str2 in str1 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)>len(str2)):\n",
    "            toRemove = all_attributes.get(str2) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str1)\n",
    "            \n",
    "            try:\n",
    "                del all_attributes[str1]\n",
    "            except:\n",
    "                pass\n",
    "            all_attributes[str1] = toRemove+toKeep\n",
    "        elif str1 in str2 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)<=len(str2)):\n",
    "            toRemove = all_attributes.get(str1) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str2)\n",
    "            try:\n",
    "                del all_attributes[str2]\n",
    "            except:\n",
    "                pass\n",
    "            all_attributes[str2] = toRemove+toKeep\n",
    "            \n",
    "        i+=1\n",
    "    new_dic = {} #reverse dict so that given any name, can find base\n",
    "    for k,v in all_attributes.items():\n",
    "        for x in v:\n",
    "            new_dic.setdefault(x,[]).append(k)\n",
    "    #now have dict of base books with values of similar titles\n",
    "        \n",
    "    all_attributes_dict = dict(zip(all_attributes.keys(), np.arange(len(all_attributes)))) #create dict for all entities\n",
    "    \n",
    "    \n",
    "    guests = [doc.user_data[\"guest\"] for doc in docs]\n",
    "    guests_dict = dict(zip(guests, np.arange(len(all_attributes), len(all_attributes)+len(guests))))#create dict for all main\n",
    "    \n",
    "#     #now create dict for all edges by going from every doc's mentions of attribute, and adding edge from guest to attribute\n",
    "    \n",
    "    i = len(guests) + len(all_attributes) #make keys for final edges\n",
    "\n",
    "    edges = []\n",
    "    for doc in docs:\n",
    "        current_name = guests_dict.get(doc.user_data[\"guest\"]) #get graph id for each guest in doc\n",
    "        for mention in doc.user_data[atts]:\n",
    "            if new_dic.get(mention):\n",
    "#                 print(new_dic.get(mention))\n",
    "                edges.append((current_name, all_attributes_dict.get(new_dic.get(mention)[0]))) #from speaker to mention of base book\n",
    "    edges_dict=dict(zip(edges, np.arange(i, i+len(edges))))\n",
    "    \n",
    "    return all_attributes_dict, guests_dict, edges_dict\n",
    "\n",
    "all_attributes_dict, guests_dict, edges_dict = att_to_csv(docs1, \"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alexander Fridman'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1[0].user_data[\"guest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = urllib.parse.quote_plus('Empyr3an')\n",
    "password = urllib.parse.quote_plus(\"B4ldr1c7@1\")\n",
    "\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://{}:{}@cluster0.4pec2.mongodb.net/Checkra?retryWrites=true&w=majority\".format(username, password))\n",
    "db = cluster[\"development\"]\n",
    "collection = db[\"books\"]\n",
    "\n",
    "# db.create_collection(\"lmini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"lex\"].delete_many({})\n",
    "db[\"lex\"].insert_many([doc.user_data for doc in docs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love, Evolution, and the Human Brain Lisa Feldman Barrett\n",
      "Quantum Computing Scott Aaronson\n",
      "Economics of AI, Social Networks, and Technology Erik Brynjolfsson\n",
      "Microsoft CTO Kevin Scott\n",
      "Psychedelics Matthew Johnson\n",
      "Reinforcement Learning and the Future of AI Michael Littman\n",
      "Deep Learning, Education, and Real-World AI Andrew Ng\n",
      "The Hard Problem of Consciousness David Chalmers\n",
      "Cruise Automation Kyle Vogt\n",
      "Thinking Fast and Slow, Deep Learning, and AI Daniel Kahneman\n",
      "Effective Altruism William MacAskill\n",
      "IBM Watson, Jeopardy & Deep Conversations with AI David Ferrucci\n",
      "Space Exploration, Space Suits, and Life on Mars Dava Newman\n",
      "Algorithms, TeX, Life, and The Art of Computer Programming Donald Knuth\n",
      "Neuralink, AI, Autopilot, and the Pale Blue Dot Elon Musk\n",
      "Tesla Autopilot Elon Musk\n",
      "The Art of Fighting and the Pursuit of Excellence John Clarke\n",
      "Universal Artificial Intelligence, AIXI, and AGI Marcus Hutter\n",
      "Physics View of the Mind and Neurobiology John Hopfield\n",
      "Stack Overflow and Coding Horror Jeff Atwood\n",
      "Meaning of Life, the Universe, and Everything Manolis Kellis\n",
      "Simulation and Superintelligence Nick Bostrom\n",
      "Ex Machina, Devs, Annihilation, and the Poetry of Science Alex Garland\n",
      "Google Eric Schmidt\n",
      "Revolutionary Ideas in Science, Math, and Society Eric Weinstein\n",
      "Keras, Deep Learning, and the Progress of AI François Chollet\n",
      "Linear Algebra, Deep Learning, Teaching, and MIT OpenCourseWare Gilbert Strang\n",
      "Toward a Hybrid of Deep Learning and Symbolic AI Gary Marcus\n",
      "Chess, Deep Blue, AI, and Putin Garry Kasparov\n",
      "Adobe Research Gavin Miller\n",
      "Comma.ai, OpenPilot, and Autonomous Vehicles George Hotz\n",
      "3Blue1Brown and the Beauty of Mathematics Grant Sanderson\n",
      "OpenAI and AGI Greg Brockman\n",
      "Python Guido van Rossum\n",
      "Spotify Gustav Soderstrom\n",
      "Machine Learning, Recommender Systems, and the Future of AI Michael I. Jordan\n",
      "Cosmos, Carl Sagan, Voyager, and the Beauty of Science Ann Druyan\n",
      "Quantum Gravity and Einstein’s Unfinished Revolution Lee Smolin\n",
      "Ethereum, Cryptocurrency, and the Future of Money Vitalik Buterin\n",
      "Human-Robot Interaction and Reward Engineering Anca Dragan\n",
      "Leadership, Hard Work, Optimism and the Infinite Game Simon Sinek\n",
      "Physics of Consciousness and the Infinite Universe Roger Penrose\n",
      "AlphaGo, AlphaZero, and Deep Reinforcement Learning David Silver\n",
      "Evolution, Intelligence, Simulation, and Memes Richard Dawkins\n",
      "Geometric Unity and the Call for New Ideas, Leaders & Institutions Eric Weinstein\n",
      "Cellular Automata, Computation, and Physics Stephen Wolfram\n",
      "Generative Adversarial Networks (GANs) Ian Goodfellow\n",
      "Thousand Brains Theory of Intelligence Jeff Hawkins\n",
      "fast.ai Deep Learning Courses and Research Jeremy Howard\n",
      "Supersymmetry, String Theory and Proving Einstein Right Jim Gates\n",
      "Moore’s Law, Microprocessors, Abstractions, and First Principles Jim Keller\n",
      "Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI Judea Pearl\n",
      "Godel Machines, Meta-Learning, and LSTMs Juergen Schmidhuber\n",
      "AI Superpo Kai-Fu Lee\n",
      "Quantum Mechanics, String Theory, and Black Holes Leonard Susskind\n",
      "Lockheed Martin Keoki Jackson\n",
      "Life 3.0 Max Tegmark\n",
      "Concepts, Analogies, Common Sense & Future of AI Melanie Mitchell\n",
      "Reinforcement Learning, Planning, and Robotics Leslie Kaelbling\n",
      "Algorithmic Fairness, Bias, Privacy, and Ethics in Machine Learning Michael Kearns\n",
      "Vsauce Michael Stevens\n",
      "CPU times: user 5.6 s, sys: 143 ms, total: 5.74 s\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))\n",
    "# with open(\"cucumber/first_doc.bin\", \"rb\") as w: #loading in\n",
    "#     new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "# docs1 += list(new_docs1.get_docs(nlp.vocab))\n",
    "for doc in docs1:\n",
    "    print(doc.user_data[\"title\"], doc.user_data[\"guest\"])\n",
    "# print(docs1[5].user_data[\"books\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each category of ents, generate csv files for ents corresponding to people\n",
    "\n",
    "att_to_csv(docs1, \"people\") \n",
    "# att_to_csv(docs1, \"places\")\n",
    "# att_to_csv(docs1, \"books\")\n",
    "# print(sorted(set([item for sublist in docs1 for item in sublist.user_data[\"people\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate megagraph for book ents\n",
    "edges = pd.read_csv(\"booksEdges.csv\", sep=',').drop(\"id\", axis=1).values.tolist()\n",
    "nodes = pd.read_csv('booksNodes.csv',sep=',').set_index(\"id\").to_dict(\"index\")\n",
    "\n",
    "source = [nodes.get(e[0])[\"name\"] for e in edges]\n",
    "target = [nodes.get(e[1])[\"name\"] for e in edges]\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':np.ones(len(source))})\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()\n",
    "\n",
    "#following are other example graphviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"Bible\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"Matthew Johnson\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['source']==\"Matthew Johnson\" )| (kg_df['source']==\"Ryan Hall\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['target']==\"Crime and Punishment\" )| (kg_df['target']==\"Atlas Shrugged\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
