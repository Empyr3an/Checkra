{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "# from checkra import happyscribe_scrape as happy\n",
    "\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "html_location = \"Archive/happyscribe_Lex_Fridman_html.txt\"\n",
    "\n",
    "# happy.update_transcripts(html_location, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634\n",
      "0 [1.0, 176, [0, 176]]\n",
      "1 [6.0, 362, [176, 538]]\n",
      "2 [8.0, 191, [538, 729]]\n",
      "3 [0.0, 282, [729, 1011]]\n",
      "4 [5.0, 282, [1011, 1293]]\n",
      "5 [8.0, 128, [1293, 1421]]\n",
      "6 [3.0, 207, [1421, 1628]]\n",
      "7 [6.0, 178, [1628, 1806]]\n",
      "8 [7.0, 192, [1806, 1998]]\n",
      "9 [0.0, 177, [1998, 2175]]\n",
      "10 [9.0, 190, [2175, 2365]]\n",
      "11 [4.0, 215, [2365, 2580]]\n",
      "12 [8.0, 78, [2580, 2658]]\n",
      "CPU times: user 28.8 s, sys: 5.31 s, total: 34.1 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from checkra.text_clean import text_fix, strip_accents\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from checkra.nlp_pipeline import nlp\n",
    "from checkra.topics.confidence_model import load_confidences, full_condense, generate_model\n",
    "\n",
    "doc = nlp(text_fix(open(\"3Lex/#103|Ben_Goertzel|Artificial_General_Intelligence.txt\").read()))\n",
    "sents = [sent.text for sent in doc.sents]\n",
    "\n",
    "\n",
    "# print(doc.user_data.keys())\n",
    "dictionary, model = generate_model(doc.text, 2700)\n",
    "condensed = load_confidences(doc.text, dictionary, model, sents) #generate initial topics + confidences, then smooth by filling in empty values and averaging\n",
    "print(len(condensed))\n",
    "fill = full_condense(condensed, len(sents), len(doc))\n",
    "for ind, i in enumerate(fill):\n",
    "    print(ind, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from checkra.topics.preprocess import parallel_process, preprocess\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_confidences(file, dictionary, model, sents):#generates array with topic & confidence for each sentence\n",
    "    sent_topics = np.zeros(len(sents))\n",
    "    for i in range(len(sent_topics)): #for each sentence assign a topic\n",
    "        bow_vector=dictionary.doc2bow(preprocess(sents[i]))\n",
    "        try:\n",
    "            sent_pred = sorted(model[bow_vector], key=lambda tup: -1*tup[1])[0]\n",
    "            if sent_pred[1]>.54995 and sents[i].count(\" \")>3: #adds only very confident sentences to list and sents with atleast 3 words\n",
    "                sent_topics[i] = sent_pred[0]\n",
    "            else:\n",
    "                sent_topics[i] = -1\n",
    "        except:\n",
    "            print(\"touched\")\n",
    "            sent_topics[i] = -1\n",
    "            \n",
    "    i = 0\n",
    "    while i <len(sent_topics): #fill in values for sentences with no topic using neighborhood\n",
    "        if sent_topics[i]==-1: #if sentence doesn't have a topic\n",
    "            interval = 3\n",
    "            nhood = np.array([a for a in sent_topics[max(0, i-interval):min(len(sents), i+interval)] if a!=-1])\n",
    "            \n",
    "            while len(nhood)==0: #incase entire interval is empty, increase interval range\n",
    "                interval+=1\n",
    "                nhood =  np.array([a for a in sent_topics[max(0, i-interval):min(len(sents), i+interval)] if a!=-1])#neighborhood of 6 points around current point\n",
    "            sent_topics[i] = statistics.mode(nhood)\n",
    "            \n",
    "        i+=1\n",
    "    stream_data = []\n",
    "    i = 0\n",
    "    while i<len(sent_topics):\n",
    "        stream_data.append([sent_topics[i], 1, [i,i+1]]) #(inclusive, exclusive)\n",
    "        i+=1\n",
    "        \n",
    "    return stream_data\n",
    "\n",
    "\n",
    "def condense_stream(sent_data):\n",
    "    i = 0\n",
    "    while i<len(sent_data)-1: #collect length of same topics that occur together\n",
    "        if sent_data[i][0] == sent_data[i+1][0]: #if same as previous topic\n",
    "            sent_data[i][2][1] = sent_data[i+1][2][1]\n",
    "            sent_data[i][1] = sent_data[i][2][1] - sent_data[i][2][0]\n",
    "            sent_data.pop(i+1)\n",
    "        else:\n",
    "            i+=1\n",
    "    return sent_data\n",
    "\n",
    "def trim_fill_gaps(sent_data, trim, sent_count, condense=lambda a: a):\n",
    "    \n",
    "    trimmed = [section for section in sent_data if section[1]>trim] \n",
    "    #keep sections of atleast a certain number of repeats\n",
    "    \n",
    "\n",
    "    trimmed[0][-1][0] = 0 # stretch first topic to beginning of podcast\n",
    "    trimmed[0][1] = trimmed[0][-1][1] - trimmed[0][-1][0]\n",
    "    i = 0\n",
    "    while i <(len(trimmed)-1): #fill gaps between filtered numbers by readjusting counts and ranges\n",
    "        if trimmed[i][2][1]!=trimmed[i+1][2][0]:\n",
    "            diff = (trimmed[i+1][2][0]-trimmed[i][2][1])/2\n",
    "    \n",
    "\n",
    "            trimmed[i+1][2][0] -= int(diff)\n",
    "            trimmed[i][2][1] += math.ceil(diff)\n",
    "\n",
    "            trimmed[i+1][1] = trimmed[i+1][2][1]-trimmed[i+1][2][0]\n",
    "            trimmed[i][1] = trimmed[i][2][1]-trimmed[i][2][0]\n",
    "        i+=1\n",
    "    \n",
    "    trimmed[-1][-1][1] = sent_count #sent last number equal to end of podcast\n",
    "    trimmed[-1][1] = trimmed[-1][-1][1] - trimmed[-1][-1][0]\n",
    "    \n",
    "    return condense(trimmed)\n",
    "\n",
    "def full_condense(original, sent_len): #keep trimming with various lengths \n",
    "    condensed = copy.deepcopy(original)\n",
    "    filled = trim_fill_gaps(condensed, 1, sent_len, condense_stream) \n",
    "    i=2\n",
    "    #trim until max number of topics are reached, or all topics are alteast 2% of entire podcast\n",
    "    while len(filled)>math.log(len(doc))*1.5 or any(count<len(sents)/50 for count in np.array(filled)[:,1]):\n",
    "        filled = trim_fill_gaps(filled, i, sent_len, condense_stream)\n",
    "        i+=1\n",
    "    return filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# confi = load_confidences(doc.text, dictionary, model, sents, condense_stream)\n",
    "# for i in confi:\n",
    "#     print(i)\n",
    "dictionary, model = generate_model(doc.text, 2700)\n",
    "condensed = condense_stream(load_confidences(doc.text, dictionary, model, sents)) #generate initial topics + confidences, then smooth by filling in empty values and averaging\n",
    "print(len(condensed))\n",
    "fill = full_condense(condensed, len(sents))\n",
    "for ind, i in enumerate(fill):\n",
    "    print(ind, i)\n",
    "# for ind, i in enumerate(condensed):\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0, 2, [1, 3]]\n",
      "[9.0, 171, [4, 175]]\n",
      "[0.0, 7, [177, 184]]\n",
      "[9.0, 2, [184, 186]]\n",
      "[0.0, 164, [186, 350]]\n",
      "[3.0, 108, [352, 460]]\n",
      "[0.0, 9, [462, 471]]\n",
      "[3.0, 88, [471, 559]]\n",
      "[6.0, 2, [561, 563]]\n",
      "[7.0, 101, [564, 665]]\n",
      "[3.0, 6, [665, 671]]\n",
      "[7.0, 7, [676, 683]]\n",
      "[1.0, 5, [683, 688]]\n",
      "[7.0, 24, [690, 714]]\n",
      "[8.0, 3, [714, 717]]\n",
      "[7.0, 26, [719, 745]]\n",
      "[8.0, 47, [750, 797]]\n",
      "[0.0, 2, [797, 799]]\n",
      "[8.0, 135, [799, 934]]\n",
      "[1.0, 5, [934, 939]]\n",
      "[8.0, 21, [939, 960]]\n",
      "[1.0, 2, [962, 964]]\n",
      "[8.0, 5, [966, 971]]\n",
      "[1.0, 203, [971, 1174]]\n",
      "[0.0, 2, [1181, 1183]]\n",
      "[1.0, 10, [1184, 1194]]\n",
      "[7.0, 120, [1194, 1314]]\n",
      "[1.0, 3, [1321, 1324]]\n",
      "[7.0, 131, [1324, 1455]]\n",
      "[6.0, 2, [1455, 1457]]\n",
      "[7.0, 74, [1457, 1531]]\n",
      "[8.0, 18, [1534, 1552]]\n",
      "[7.0, 7, [1552, 1559]]\n",
      "[8.0, 57, [1559, 1616]]\n",
      "[7.0, 2, [1616, 1618]]\n",
      "[8.0, 18, [1618, 1636]]\n",
      "[0.0, 4, [1639, 1643]]\n",
      "[8.0, 66, [1643, 1709]]\n",
      "[6.0, 129, [1709, 1838]]\n",
      "[9.0, 3, [1838, 1841]]\n",
      "[6.0, 5, [1841, 1846]]\n"
     ]
    }
   ],
   "source": [
    "for i in condense_stream([section for section in condensed if section[1]>1]):\n",
    "# for i in condense_stream(condensed):\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [9.0, 176, [0, 176]]\n",
      "1 [0.0, 175, [176, 351]]\n",
      "2 [3.0, 211, [351, 562]]\n",
      "3 [7.0, 186, [562, 748]]\n",
      "4 [8.0, 223, [748, 971]]\n",
      "5 [1.0, 223, [971, 1194]]\n",
      "6 [7.0, 339, [1194, 1533]]\n",
      "7 [8.0, 176, [1533, 1709]]\n",
      "8 [6.0, 137, [1709, 1846]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def trim_fill_gaps(sent_data, trim, sent_count, condense=lambda a: a):\n",
    "    \n",
    "    \n",
    "    print(\"method prefilder\",len(sent_data))\n",
    "    trimmed = [section for section in sent_data if section[1]>trim] #keep sections of atleast a certain number of repeats\n",
    "    print(\"method postfilter\",len(sent_data))\n",
    "#     for i in trimmed:\n",
    "#         print(i)\n",
    "#     for ind, i in enumerate(sent_data):\n",
    "#         print(ind, i)\n",
    "\n",
    "\n",
    "    trimmed[0][-1][0] = 0\n",
    "    trimmed[0][1] = trimmed[0][-1][1] - trimmed[0][-1][0]\n",
    "    i = 0\n",
    "    while i <(len(trimmed)-1): #fill gaps between filtered numbers\n",
    "        if trimmed[i][2][1]!=trimmed[i+1][2][0]:\n",
    "            diff = (trimmed[i+1][2][0]-trimmed[i][2][1])/2\n",
    "            print(trimmed[i], \"\\n\",trimmed[i+1], diff, \"sdfdsf\\n\")\n",
    "    \n",
    "\n",
    "            trimmed[i+1][2][0] -= int(diff)\n",
    "            trimmed[i][2][1] += math.ceil(diff)\n",
    "\n",
    "            trimmed[i+1][1] = trimmed[i+1][2][1]-trimmed[i+1][2][0]\n",
    "            trimmed[i][1] = trimmed[i][2][1]-trimmed[i][2][0]\n",
    "            print(trimmed[i], \"\\n\",trimmed[i+1], \"\\n\\n\\n\")\n",
    "        i+=1\n",
    "    \n",
    "    trimmed[-1][-1][1] = sent_count\n",
    "    trimmed[-1][1] = trimmed[-1][-1][1] - trimmed[-1][-1][0]\n",
    "\n",
    "    \n",
    "    \n",
    "#     if trim!=0:\n",
    "#         for i in sent_data:\n",
    "#             print(i)\n",
    "#         print(\"\\t\\t\\tdone\")\n",
    "#     for ind, i in enumerate(condense(sent_data)):\n",
    "#         print(ind, i)\n",
    "#         print(a)\n",
    "#     print(\"while done\")\n",
    "    \n",
    "    return condense(trimmed)\n",
    "\n",
    "def full_condense(original, sent_len):\n",
    "    condensed = copy.deepcopy(original)\n",
    "    print(\"start\",len(condensed))\n",
    "    filled = fill_gaps(condensed, 1, sent_len, condense_stream)\n",
    "    \n",
    "#     print(\"\\t\\t\\t\",1, len(condensed), len(filled))\n",
    "#     filled = fill_gaps(filled, 2, sent_len, condense_stream)\n",
    "#     print(\"\\t\\t\\t\",2, len(condensed), len(filled))\n",
    "#     filled = fill_gaps(filled, 3, sent_len, condense_stream)\n",
    "#     print(\"\\t\\t\\t\",3, len(condensed), len(filled))\n",
    "#     for ind, j in enumerate(condense_stream(filled)):\n",
    "#         print(j)\n",
    "    i=2\n",
    "    print(\"\\t\\t\\t\\tstart\")\n",
    "    \n",
    "    while len(filled)>math.log(len(doc))*1.5 or any(count<len(sents)/50 for count in np.array(filled)[:,1]):\n",
    "        print(\"\\t\\t\\tbefore\",i, len(filled))\n",
    "        filled = fill_gaps(filled, i, sent_len, condense_stream)\n",
    "        print(\"\\t\\t\\tafter\",i, len(filled))\n",
    "#         for ind, j in enumerate(filled):\n",
    "#             print(j)\n",
    "        i+=1\n",
    "    return filled\n",
    "\n",
    "fill = full_condense(condensed, len(sents))\n",
    "for ind, i in enumerate(condense_stream(condensed)):\n",
    "    print(ind, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"starting\")\n",
    "folder=\"3Lex\"\n",
    "podcast_host = \"Lex Fridman\"\n",
    "\n",
    "\n",
    "# docs, doc_bin = process_folder_to_docs(\"5Lex\", podcast_host)\n",
    "onlyfiles=[(text_fix(open(folder+\"/\"+file).read()), file) for file in listdir(folder) if isfile(join(folder, file))]\n",
    "docs = [] #list of docs \n",
    "\n",
    "for doc, name in tqdm(nlp.pipe(onlyfiles, as_tuples=True)): #piping all collection of docs to make doclist and docbin\n",
    "    #each doc contains hostname, guest, title, entities mentioned, and summary\n",
    "    doc.user_data[\"url\"] = main+url+\"/\"+strip_accents(name.replace(\"|\",\"-\").replace(\"#\",\"\").replace(\".txt\",\"\").replace(\"_\",\"-\").replace(\",\",\"\").lower())\n",
    "    name = re.split(\"[\\|]\",name)\n",
    "    name=name[1:] if name[0][1:].isdigit() else name # store name of guest and topic, add to doc user data\n",
    "    \n",
    "    doc.user_data[\"host\"] = podcast_host\n",
    "    doc.user_data[\"guest\"]= str(name[0]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"title\"]= str(name[1][:-4]).replace(\"_\",\" \")\n",
    "    docs.append(doc)\n",
    "\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, doc in enumerate(docs):\n",
    "    print(ind, doc.user_data[\"guest\"], doc.user_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].user_data[\"traits\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "count = 0\n",
    "not_processed = []\n",
    "for ind, file in enumerate(os.listdir(\"7Lex\")):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file2 = file.replace(\"_\", \" \").replace(\"|\", \" \").replace(\".txt\", \" \")\n",
    "    #     print(ind, file) \n",
    "        processed = False\n",
    "\n",
    "\n",
    "        for doc in docs:\n",
    "            if doc.user_data[\"guest\"] in file2 and doc.user_data[\"title\"] in file2:\n",
    "    #             os.rename(\"5Lex/\"+file, \"7Lex/\"+file)\n",
    "                \n",
    "                processed = True\n",
    "                count+=1\n",
    "                break\n",
    "        if processed:\n",
    "            os.rename(\"7Lex/\"+file, \"5Lex/\"+file)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "username = urllib.parse.quote_plus('Empyr3an')\n",
    "password = urllib.parse.quote_plus(\"B4ldr1c7@1\")\n",
    "\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://{}:{}@cluster0.4pec2.mongodb.net/Checkra?retryWrites=true&w=majority\".format(username, password))\n",
    "db = cluster[\"production\"]\n",
    "collection = db[\"lex\"]\n",
    "\n",
    "# db.create_collection(\"lmini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried = list(db[\"lex\"].find({},{\"_id\":0,\"traits.Events\":1}))\n",
    "all_ents = [ent for doc in queried for ent in doc[\"traits\"][\"Events\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"lex\"].drop()\n",
    "db[\"lex\"].insert_many([doc.user_data for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_atts = []\n",
    "for doc in list(collection.find({\"books\":{\"$ne\":[]}})):\n",
    "    print(doc[\"guest\"], doc[att])\n",
    "    for a in doc[att]:\n",
    "        all_atts.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "\n",
    "    doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "# with open(\"cucumber/Lexnumbered.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "#     f.write(doc_bin.to_bytes())\n",
    "\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "att_to_csv(docs1, \"people\") #to get csv of edges\n",
    "\n",
    "#generate megagraph for book ents\n",
    "edges = pd.read_csv(\"booksEdges.csv\", sep=',').drop(\"id\", axis=1).values.tolist()\n",
    "nodes = pd.read_csv('booksNodes.csv',sep=',').set_index(\"id\").to_dict(\"index\")\n",
    "\n",
    "source = [nodes.get(e[0])[\"name\"] for e in edges]\n",
    "target = [nodes.get(e[1])[\"name\"] for e in edges]\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':np.ones(len(source))})\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()\n",
    "\n",
    "#following are other example graphviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"Bible\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
