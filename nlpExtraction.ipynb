{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "CPU times: user 13 s, sys: 628 ms, total: 13.6 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "#import in all libraries and methods\n",
    "# %run -i 'myimports.py'\n",
    "# %run -i 'datagathering.py'\n",
    "# %run -i 'nlp_processing.py' #imports packages and methods\n",
    "from checkra import happyscribe_scrape as happy\n",
    "\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "podcast_host = \"Lex Fridman\"\n",
    "html_location = \"Archive/happyscribe_Lex_Fridman_html.txt\"\n",
    "\n",
    "happy.update_transcripts(html_location, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkra' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/podcast/nlp_processing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkra' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run -i 'myimports.py'\n",
    "%run -i 'nlp_processing.py' #imports packages and methods\n",
    "\n",
    "\n",
    "doc = nlp(text_clean.text_fix(open(\"3Lex/#107|Peter_Singer|Suffering_in_Humans,_Animals,_and_AI.txt\").read()))\n",
    "\n",
    "print(doc.user_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_algo_timestamps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36msubtopics\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mone_topic_confi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_confidences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#generate initial topics + confidences, then smooth by filling in empty values and averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_algo_timestamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_topic_confi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#algo generated timestamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stamps\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstamps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sent_count\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_algo_timestamps' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(text_fix(open(\"3Lex/#101|Joscha_Bach|Artificial_Consciousness_and_the_Nature_of_Reality.txt\").read()))\n",
    "# doc = nlp(text_fix(open(\"3Lex/#103|Ben_Goertzel|Artificial_General_Intelligence.txt\").read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['People', 'Books', 'Topics', 'Places', 'Products/Companies', 'Events', 'Laws', 'Identity Groups', 'All Mentions'])\n"
     ]
    }
   ],
   "source": [
    "print(doc.user_data[\"traits\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation with Ben Gursel, one of the most interesting minds in the artificial intelligence community.\n",
      "he is the founder of Singularity, not designer of Open Kagi Framework, formerly a director of research at the Machine Intelligence Research Institute and chief scientist of HANDSON Robotics, the company that created the Sophea robot.\n",
      "He has been a central figure in the Ajai community for many years, including in his organizing and contributing to the conference and artificial general intelligence, the 2020 version of which is actually happening this week, Wednesday, Thursday and Friday.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:01, 74.94s/it] /ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "31it [14:10, 23.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation with Stephen Wolfram, his second time in the podcast.\n",
      "he is a computer scientist, mathematician, theoretical physicist and the founder and CEO of Wolfram Research, a company behind Mathematica, Wolfram Alpha, Wolfram Language and the new Wolfram Physics Project.\n",
      "he is the author of several books, including A New Kind of Science and the new book, A Project to Find the Fundamental Theory of Physics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [20:27, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 10min 51s, sys: 7min 34s, total: 18min 25s\n",
      "Wall time: 20min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "folder=\"3Lex\"\n",
    "# docs, doc_bin = process_folder_to_docs(\"5Lex\", podcast_host)\n",
    "onlyfiles=[(text_fix(open(folder+\"/\"+file).read()), file) for file in listdir(folder) if isfile(join(folder, file))]\n",
    "docs = [] #list of docs \n",
    "\n",
    "for doc, name in tqdm(nlp.pipe(onlyfiles, as_tuples=True)): #piping all collection of docs to make doclist and docbin\n",
    "    #each doc contains hostname, guest, title, entities mentioned, and summary\n",
    "    name = re.split(\"[\\|]\",name)\n",
    "    name=name[1:] if name[0][1:].isdigit() else name # store name of guest and topic, add to doc user data\n",
    "    \n",
    "    doc.user_data[\"host\"] = podcast_host\n",
    "    doc.user_data[\"guest\"]= str(name[0]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"title\"]= str(name[1][:-4]).replace(\"_\",\" \")\n",
    "    doc = subtopics(doc)\n",
    "    docs.append(doc)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI Winters']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.user_data[\"traits\"][\"Events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ben Goertzel\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].user_data[\"guest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Ben Goertzel Artificial General Intelligence\n",
      "1 Steven Pressfield The War of Art\n",
      "2 Alexander Fridman My Dad, the Plasma Physicist\n",
      "3 Robert Langer Edison of Medicine\n",
      "4 Joscha Bach Artificial Consciousness and the Nature of Reality\n",
      "5 Yaron Brook Ayn Rand and the Philosophy of Objectivism\n",
      "6 John Clarke The Art of Fighting and the Pursuit of Excellence\n",
      "7 David Patterson Computer Architecture and Data Storage\n",
      "8 Matt Botvinick Neuroscience, Psychology, and AI at DeepMind\n",
      "9 Matthew Johnson Psychedelics\n",
      "10 Andrew Huberman Neuroscience of Optimal Performance\n",
      "11 Peter Singer Suffering in Humans, Animals, and AI\n",
      "12 Sergey Levine Robotics and Machine Learning\n",
      "13 Brian Kernighan UNIX, C, AWK, AMPL, and Go Programming\n",
      "14 Jitendra Malik Computer Vision\n",
      "15 Ian Hutchinson Nuclear Fusion, Plasma Physics, and Religion\n",
      "16 Lisa Feldman Barrett Love, Evolution, and the Human Brain\n",
      "17 Manolis Kellis Human Genome and Evolutionary Dynamics\n",
      "18 Erik Brynjolfsson Economics of AI, Social Networks, and Technology\n",
      "19 Manolis Kellis Meaning of Life, the Universe, and Everything\n",
      "20 Dileep George Brain-Inspired AI\n",
      "21 Michael Littman Reinforcement Learning and the Future of AI\n",
      "22 Richard Karp Algorithms and Computational Complexity\n",
      "23 Russ Tedrake Underactuated Robotics, Control, Dynamics and Touch\n",
      "24 David Eagleman Neuroplasticity and the Livewired Brain\n",
      "25 Sheldon Solomon Death and Meaning\n",
      "26 Eugenia Kuyda Friendship with an AI Companion\n",
      "27 Sara Seager Search for Planets and Life Outside Our Solar System\n",
      "28 David Fravor UFOs, Aliens, Fighter Jets, and Aerospace Engineering\n",
      "29 Manolis Kellis Biology of Disease\n",
      "30 Manolis Kellis Origin of Life, Humans, Ideas, Suffering, and Happiness\n",
      "31 Stephen Wolfram Fundamental Theory of Physics, Life, and the Universe\n",
      "32 Grant Sanderson Math, Manim, Neural Networks & Teaching with 3Blue1Brown\n",
      "33 Dan Carlin Hardcore History\n",
      "34 Ryan Hall Martial Arts and the Philosophy of Violence, Power, and Grace\n",
      "35 James Gosling Java, JVM, Emacs, and the Early Days of Computing\n",
      "36 Joe Rogan Conversations, Ideas, Love, Freedom & The Joe Rogan Experience\n",
      "37 Michael Malice Anarchy, Democracy, Libertarianism, Love, and Trolling\n",
      "38 Lisa Feldman Barrett Counterintuitive Ideas About How the Brain Works\n",
      "39 Scott Aaronson Computational Complexity and Consciousness\n",
      "40 Chris Lattner The Future of Computing and Programming Languages\n",
      "41 François Chollet Measures of Intelligence\n",
      "42 George Hotz Hacking the Simulation & Learning to Drive with Neural Nets\n",
      "43 Eric Weinstein On the Nature of Good and Evil, Genius and Madness\n",
      "44 Charles Isbell Computing, Interactive AI, and Race in America\n",
      "45 Alex Filippenko Supernovae, Dark Energy, Aliens & the Expanding Universe\n"
     ]
    }
   ],
   "source": [
    "for ind, doc in enumerate(docs):\n",
    "    print(ind, doc.user_data[\"guest\"], doc.user_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['People', 'Books', 'Topics', 'Places', 'Products/Companies', 'Events', 'Laws', 'Identity Groups'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].user_data[\"traits\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "count = 0\n",
    "not_processed = []\n",
    "for ind, file in enumerate(os.listdir(\"7Lex\")):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file2 = file.replace(\"_\", \" \").replace(\"|\", \" \").replace(\".txt\", \" \")\n",
    "    #     print(ind, file) \n",
    "        processed = False\n",
    "\n",
    "\n",
    "        for doc in docs:\n",
    "            if doc.user_data[\"guest\"] in file2 and doc.user_data[\"title\"] in file2:\n",
    "    #             os.rename(\"5Lex/\"+file, \"7Lex/\"+file)\n",
    "                \n",
    "                processed = True\n",
    "                count+=1\n",
    "                break\n",
    "        if processed:\n",
    "            os.rename(\"7Lex/\"+file, \"5Lex/\"+file)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 0 ns, total: 20.2 ms\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def att_to_csv(docs, atts):\n",
    "    all_atts= sorted(set([item for doc in docs for item in doc.user_data[atts]])) #all atts\n",
    "    all_attributes=dict([(x, [x]) for x in all_atts]) #dictionary of books to become key is base book, value are similar titles\n",
    "    i = 0\n",
    "#     similar_words implement dictionary to store similar words that were deleted\n",
    "    while i<len(all_attributes)-1: #remove similar or subwords\n",
    "        str1, str2 = all_atts[i], all_atts[i+1]\n",
    "        if str2 in str1 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)>len(str2)):\n",
    "            toRemove = all_attributes.get(str2) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str1)\n",
    "            \n",
    "            try:\n",
    "                del all_attributes[str1]\n",
    "            except:\n",
    "                pass\n",
    "            all_attributes[str1] = toRemove+toKeep\n",
    "        elif str1 in str2 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)<=len(str2)):\n",
    "            toRemove = all_attributes.get(str1) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str2)\n",
    "            try:\n",
    "                del all_attributes[str2]\n",
    "            except:\n",
    "                pass\n",
    "            all_attributes[str2] = toRemove+toKeep\n",
    "            \n",
    "        i+=1\n",
    "    new_dic = {} #reverse dict so that given any name, can find base\n",
    "    for k,v in all_attributes.items():\n",
    "        for x in v:\n",
    "            new_dic.setdefault(x,[]).append(k)\n",
    "    #now have dict of base books with values of similar titles\n",
    "        \n",
    "    all_attributes_dict = dict(zip(all_attributes.keys(), np.arange(len(all_attributes)))) #create dict for all entities\n",
    "    \n",
    "    \n",
    "    guests = [doc.user_data[\"guest\"] for doc in docs]\n",
    "    guests_dict = dict(zip(guests, np.arange(len(all_attributes), len(all_attributes)+len(guests))))#create dict for all main\n",
    "    \n",
    "#     #now create dict for all edges by going from every doc's mentions of attribute, and adding edge from guest to attribute\n",
    "    \n",
    "    i = len(guests) + len(all_attributes) #make keys for final edges\n",
    "\n",
    "    edges = []\n",
    "    for doc in docs:\n",
    "        current_name = guests_dict.get(doc.user_data[\"guest\"]) #get graph id for each guest in doc\n",
    "        for mention in doc.user_data[atts]:\n",
    "            if new_dic.get(mention):\n",
    "#                 print(new_dic.get(mention))\n",
    "                edges.append((current_name, all_attributes_dict.get(new_dic.get(mention)[0]))) #from speaker to mention of base book\n",
    "    edges_dict=dict(zip(edges, np.arange(i, i+len(edges))))\n",
    "    \n",
    "    return all_attributes_dict, guests_dict, edges_dict\n",
    "\n",
    "all_attributes_dict, guests_dict, edges_dict = att_to_csv(docs1, \"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alexander Fridman'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1[0].user_data[\"guest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes=dict([(x, [x]) for x in all_atts]) #dictionary of books to become key is base book, value are similar titles\n",
    "i = 0\n",
    "#     similar_words implement dictionary to store similar words that were deleted\n",
    "while i<len(all_attributes)-1: #remove similar or subwords\n",
    "    str1, str2 = all_atts[i], all_atts[i+1]\n",
    "    if str2 in str1 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)>len(str2)):\n",
    "        print(\"1\", str1, str2)\n",
    "        toRemove = all_attributes.get(str2) #list of similar words moving\n",
    "        toKeep =  all_attributes.get(str1)\n",
    "\n",
    "        try:\n",
    "            print(all_attributes.keys(),\"\\n\")\n",
    "            del all_attributes[str1]\n",
    "            print(all_attributes.keys())\n",
    "        except:\n",
    "            print(\"rip\")\n",
    "            pass\n",
    "        all_attributes[str1] = toRemove+toKeep\n",
    "    elif str1 in str2 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)<=len(str2)):\n",
    "        print(\"2\",str1, str2)\n",
    "        toRemove = all_attributes.get(str1) #list of similar words moving\n",
    "        toKeep =  all_attributes.get(str2)\n",
    "        try:\n",
    "            del all_attributes[str2]\n",
    "        except:\n",
    "            pass\n",
    "        all_attributes[str2] = toRemove+toKeep\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = urllib.parse.quote_plus('Empyr3an')\n",
    "password = urllib.parse.quote_plus(\"B4ldr1c7@1\")\n",
    "\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://{}:{}@cluster0.4pec2.mongodb.net/Checkra?retryWrites=true&w=majority\".format(username, password))\n",
    "db = cluster[\"production\"]\n",
    "collection = db[\"lex\"]\n",
    "\n",
    "# db.create_collection(\"lmini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried = list(db[\"lex\"].find({},{\"_id\":0,\"traits.Events\":1}))\n",
    "all_ents = [ent for doc in queried for ent in doc[\"traits\"][\"Events\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fdf476a2980>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[\"lex\"].drop()\n",
    "db[\"lex\"].insert_many([doc.user_data for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_atts = []\n",
    "for doc in list(collection.find({\"books\":{\"$ne\":[]}})):\n",
    "    print(doc[\"guest\"], doc[att])\n",
    "    for a in doc[att]:\n",
    "        all_atts.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love, Evolution, and the Human Brain Lisa Feldman Barrett\n",
      "Quantum Computing Scott Aaronson\n",
      "Economics of AI, Social Networks, and Technology Erik Brynjolfsson\n",
      "Microsoft CTO Kevin Scott\n",
      "Psychedelics Matthew Johnson\n",
      "Reinforcement Learning and the Future of AI Michael Littman\n",
      "Deep Learning, Education, and Real-World AI Andrew Ng\n",
      "The Hard Problem of Consciousness David Chalmers\n",
      "Cruise Automation Kyle Vogt\n",
      "Thinking Fast and Slow, Deep Learning, and AI Daniel Kahneman\n",
      "Effective Altruism William MacAskill\n",
      "IBM Watson, Jeopardy & Deep Conversations with AI David Ferrucci\n",
      "Space Exploration, Space Suits, and Life on Mars Dava Newman\n",
      "Algorithms, TeX, Life, and The Art of Computer Programming Donald Knuth\n",
      "Neuralink, AI, Autopilot, and the Pale Blue Dot Elon Musk\n",
      "Tesla Autopilot Elon Musk\n",
      "The Art of Fighting and the Pursuit of Excellence John Clarke\n",
      "Universal Artificial Intelligence, AIXI, and AGI Marcus Hutter\n",
      "Physics View of the Mind and Neurobiology John Hopfield\n",
      "Stack Overflow and Coding Horror Jeff Atwood\n",
      "Meaning of Life, the Universe, and Everything Manolis Kellis\n",
      "Simulation and Superintelligence Nick Bostrom\n",
      "Ex Machina, Devs, Annihilation, and the Poetry of Science Alex Garland\n",
      "Google Eric Schmidt\n",
      "Revolutionary Ideas in Science, Math, and Society Eric Weinstein\n",
      "Keras, Deep Learning, and the Progress of AI François Chollet\n",
      "Linear Algebra, Deep Learning, Teaching, and MIT OpenCourseWare Gilbert Strang\n",
      "Toward a Hybrid of Deep Learning and Symbolic AI Gary Marcus\n",
      "Chess, Deep Blue, AI, and Putin Garry Kasparov\n",
      "Adobe Research Gavin Miller\n",
      "Comma.ai, OpenPilot, and Autonomous Vehicles George Hotz\n",
      "3Blue1Brown and the Beauty of Mathematics Grant Sanderson\n",
      "OpenAI and AGI Greg Brockman\n",
      "Python Guido van Rossum\n",
      "Spotify Gustav Soderstrom\n",
      "Machine Learning, Recommender Systems, and the Future of AI Michael I. Jordan\n",
      "Cosmos, Carl Sagan, Voyager, and the Beauty of Science Ann Druyan\n",
      "Quantum Gravity and Einstein’s Unfinished Revolution Lee Smolin\n",
      "Ethereum, Cryptocurrency, and the Future of Money Vitalik Buterin\n",
      "Human-Robot Interaction and Reward Engineering Anca Dragan\n",
      "Leadership, Hard Work, Optimism and the Infinite Game Simon Sinek\n",
      "Physics of Consciousness and the Infinite Universe Roger Penrose\n",
      "AlphaGo, AlphaZero, and Deep Reinforcement Learning David Silver\n",
      "Evolution, Intelligence, Simulation, and Memes Richard Dawkins\n",
      "Geometric Unity and the Call for New Ideas, Leaders & Institutions Eric Weinstein\n",
      "Cellular Automata, Computation, and Physics Stephen Wolfram\n",
      "Generative Adversarial Networks (GANs) Ian Goodfellow\n",
      "Thousand Brains Theory of Intelligence Jeff Hawkins\n",
      "fast.ai Deep Learning Courses and Research Jeremy Howard\n",
      "Supersymmetry, String Theory and Proving Einstein Right Jim Gates\n",
      "Moore’s Law, Microprocessors, Abstractions, and First Principles Jim Keller\n",
      "Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI Judea Pearl\n",
      "Godel Machines, Meta-Learning, and LSTMs Juergen Schmidhuber\n",
      "AI Superpo Kai-Fu Lee\n",
      "Quantum Mechanics, String Theory, and Black Holes Leonard Susskind\n",
      "Lockheed Martin Keoki Jackson\n",
      "Life 3.0 Max Tegmark\n",
      "Concepts, Analogies, Common Sense & Future of AI Melanie Mitchell\n",
      "Reinforcement Learning, Planning, and Robotics Leslie Kaelbling\n",
      "Algorithmic Fairness, Bias, Privacy, and Ethics in Machine Learning Michael Kearns\n",
      "Vsauce Michael Stevens\n",
      "CPU times: user 5.6 s, sys: 143 ms, total: 5.74 s\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "\n",
    "    doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "# with open(\"cucumber/Lexnumbered.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "#     f.write(doc_bin.to_bytes())\n",
    "\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_to_csv(docs1, \"people\") #to get csv of edges\n",
    "\n",
    "#generate megagraph for book ents\n",
    "edges = pd.read_csv(\"booksEdges.csv\", sep=',').drop(\"id\", axis=1).values.tolist()\n",
    "nodes = pd.read_csv('booksNodes.csv',sep=',').set_index(\"id\").to_dict(\"index\")\n",
    "\n",
    "source = [nodes.get(e[0])[\"name\"] for e in edges]\n",
    "target = [nodes.get(e[1])[\"name\"] for e in edges]\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':np.ones(len(source))})\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()\n",
    "\n",
    "#following are other example graphviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"Bible\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
