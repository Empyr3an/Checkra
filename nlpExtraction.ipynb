{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "CPU times: user 21.9 s, sys: 1.55 s, total: 23.5 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "#import in all libraries and methods\n",
    "%run -i 'myimports.py'\n",
    "%run -i 'datagathering.py'\n",
    "%run -i 'nlp_processing.py' #imports packages and methods\n",
    "# %run -i 'old_nlp.py'\n",
    "%run -i 'timestamp_generation.py'\n",
    "\n",
    "main = \"https://www.happyscribe.com\"                \n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "podcast_host = \"Lex Fridman\"\n",
    "html_location = \"Archive/happyscribe_Lex_Fridman_html.txt\"\n",
    "\n",
    "# update_transcripts(html_location, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /ilab/users/hs884/.local/lib/python3.8/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 3.12 s, total: 19.2 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(text_fix(open(\"3Lex/#101|Joscha_Bach|Artificial_Consciousness_and_the_Nature_of_Reality.txt\").read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['People', 'Books', 'Topics', 'Places', 'Products/Companies', 'Events', 'Laws', 'Identity Groups'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.user_data[\"traits\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:58, 21.67s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python38/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, kwargs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36msubtopics\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mone_topic_confi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_confidences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_completion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#generate initial topics + confidences, then smooth by filling in empty values and averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/podcast/timestamp_generation.py\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(text, doc_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mbow_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_pod\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#dict for how many times each word appears\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#lda topic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute LDA over an empty collection (no terms)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"starting\")\n",
    "podcast_name=\"6Lex\"\n",
    "# docs, doc_bin = process_folder_to_docs(\"5Lex\", podcast_host)\n",
    "onlyfiles = folder_to_filelist(podcast_name) #get list (complete text, filename)\n",
    "doc_bin = DocBin(store_user_data=True) #docbin container for serialization\n",
    "docs = [] #list of docs \n",
    "\n",
    "for doc, name in tqdm(nlp.pipe(onlyfiles, as_tuples=True)): #piping all collection of docs to make doclist and docbin\n",
    "    #each doc contains hostname, guest, title, entities mentioned, and summary\n",
    "    name = re.split(\"[\\|]\",name)\n",
    "    name=name[1:] if name[0][1:].isdigit() else name # store name of guest and topic, add to doc user data\n",
    "    \n",
    "    doc.user_data[\"host\"] = podcast_host\n",
    "    doc.user_data[\"guest\"]= str(name[0]).replace(\"_\",\" \")\n",
    "    doc.user_data[\"title\"]= str(name[1][:-4]).replace(\"_\",\" \")\n",
    "    doc = subtopics(doc)\n",
    "    docs.append(doc)\n",
    "    doc_bin.add(doc) #add doc to list and bin\n",
    "    \n",
    "# with open(\"cucumber/Lexnumbered.bin\", \"wb\") as f: #write bytestream to first_doc.bin\n",
    "#     f.write(doc_bin.to_bytes())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI Winters']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.user_data[\"traits\"][\"Events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Andrew Ng Deep Learning, Education, and Real-World AI\n",
      "1 Michael I. Jordan Machine Learning, Recommender Systems, and the Future of AI\n",
      "2 Scott Aaronson Quantum Computing\n",
      "3 Marcus Hutter Universal Artificial Intelligence, AIXI, and AGI\n",
      "4 John Hopfield Physics View of the Mind and Neurobiology\n",
      "5 Alex Garland Ex Machina, Devs, Annihilation, and the Poetry of Science\n",
      "6 Ann Druyan Cosmos, Carl Sagan, Voyager, and the Beauty of Science\n",
      "7 Nick Bostrom Simulation and Superintelligence\n",
      "8 William MacAskill Effective Altruism\n",
      "9 Vitalik Buterin Ethereum, Cryptocurrency, and the Future of Money\n",
      "10 Anca Dragan Human-Robot Interaction and Reward Engineering\n"
     ]
    }
   ],
   "source": [
    "for ind, doc in enumerate(docs):\n",
    "    print(ind, doc.user_data[\"guest\"], doc.user_data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['People', 'Books', 'Topics', 'Places', 'Products/Companies', 'Events', 'Laws', 'Identity Groups'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].user_data[\"traits\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "count = 0\n",
    "not_processed = []\n",
    "for ind, file in enumerate(os.listdir(\"5Lex\")):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file2 = file.replace(\"_\", \" \").replace(\"|\", \" \").replace(\".txt\", \" \")\n",
    "    #     print(ind, file) \n",
    "        processed = False\n",
    "\n",
    "\n",
    "        for doc in docs:\n",
    "            if doc.user_data[\"guest\"] in file2 and doc.user_data[\"title\"] in file2:\n",
    "    #             os.rename(\"5Lex/\"+file, \"7Lex/\"+file)\n",
    "                \n",
    "                processed = True\n",
    "                count+=1\n",
    "                break\n",
    "        if processed:\n",
    "            os.rename(\"5Lex/\"+file, \"7Lex/\"+file)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 0 ns, total: 20.2 ms\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def att_to_csv(docs, atts):\n",
    "    all_atts= sorted(set([item for doc in docs for item in doc.user_data[atts]])) #all atts\n",
    "    all_attributes=dict([(x, [x]) for x in all_atts]) #dictionary of books to become key is base book, value are similar titles\n",
    "    i = 0\n",
    "#     similar_words implement dictionary to store similar words that were deleted\n",
    "    while i<len(all_attributes)-1: #remove similar or subwords\n",
    "        str1, str2 = all_atts[i], all_atts[i+1]\n",
    "        if str2 in str1 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)>len(str2)):\n",
    "            toRemove = all_attributes.get(str2) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str1)\n",
    "            \n",
    "            try:\n",
    "                del all_attributes[str1]\n",
    "            except:\n",
    "                pass\n",
    "            all_attributes[str1] = toRemove+toKeep\n",
    "        elif str1 in str2 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)<=len(str2)):\n",
    "            toRemove = all_attributes.get(str1) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str2)\n",
    "            try:\n",
    "                del all_attributes[str2]\n",
    "            except:\n",
    "                pass\n",
    "            all_attributes[str2] = toRemove+toKeep\n",
    "            \n",
    "        i+=1\n",
    "    new_dic = {} #reverse dict so that given any name, can find base\n",
    "    for k,v in all_attributes.items():\n",
    "        for x in v:\n",
    "            new_dic.setdefault(x,[]).append(k)\n",
    "    #now have dict of base books with values of similar titles\n",
    "        \n",
    "    all_attributes_dict = dict(zip(all_attributes.keys(), np.arange(len(all_attributes)))) #create dict for all entities\n",
    "    \n",
    "    \n",
    "    guests = [doc.user_data[\"guest\"] for doc in docs]\n",
    "    guests_dict = dict(zip(guests, np.arange(len(all_attributes), len(all_attributes)+len(guests))))#create dict for all main\n",
    "    \n",
    "#     #now create dict for all edges by going from every doc's mentions of attribute, and adding edge from guest to attribute\n",
    "    \n",
    "    i = len(guests) + len(all_attributes) #make keys for final edges\n",
    "\n",
    "    edges = []\n",
    "    for doc in docs:\n",
    "        current_name = guests_dict.get(doc.user_data[\"guest\"]) #get graph id for each guest in doc\n",
    "        for mention in doc.user_data[atts]:\n",
    "            if new_dic.get(mention):\n",
    "#                 print(new_dic.get(mention))\n",
    "                edges.append((current_name, all_attributes_dict.get(new_dic.get(mention)[0]))) #from speaker to mention of base book\n",
    "    edges_dict=dict(zip(edges, np.arange(i, i+len(edges))))\n",
    "    \n",
    "    return all_attributes_dict, guests_dict, edges_dict\n",
    "\n",
    "all_attributes_dict, guests_dict, edges_dict = att_to_csv(docs1, \"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alexander Fridman'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1[0].user_data[\"guest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = urllib.parse.quote_plus('Empyr3an')\n",
    "password = urllib.parse.quote_plus(\"B4ldr1c7@1\")\n",
    "\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://{}:{}@cluster0.4pec2.mongodb.net/Checkra?retryWrites=true&w=majority\".format(username, password))\n",
    "db = cluster[\"production\"]\n",
    "collection = db[\"lex\"]\n",
    "\n",
    "# db.create_collection(\"lmini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried = list(db[\"lex\"].find({},{\"_id\":0,\"traits.Events\":1}))\n",
    "all_ents = [ent for doc in queried for ent in doc[\"traits\"][\"Events\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fa4aefd7a80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db[\"inventory\"].drop()\n",
    "db[\"lex\"].insert_many([doc.user_data for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love, Evolution, and the Human Brain Lisa Feldman Barrett\n",
      "Quantum Computing Scott Aaronson\n",
      "Economics of AI, Social Networks, and Technology Erik Brynjolfsson\n",
      "Microsoft CTO Kevin Scott\n",
      "Psychedelics Matthew Johnson\n",
      "Reinforcement Learning and the Future of AI Michael Littman\n",
      "Deep Learning, Education, and Real-World AI Andrew Ng\n",
      "The Hard Problem of Consciousness David Chalmers\n",
      "Cruise Automation Kyle Vogt\n",
      "Thinking Fast and Slow, Deep Learning, and AI Daniel Kahneman\n",
      "Effective Altruism William MacAskill\n",
      "IBM Watson, Jeopardy & Deep Conversations with AI David Ferrucci\n",
      "Space Exploration, Space Suits, and Life on Mars Dava Newman\n",
      "Algorithms, TeX, Life, and The Art of Computer Programming Donald Knuth\n",
      "Neuralink, AI, Autopilot, and the Pale Blue Dot Elon Musk\n",
      "Tesla Autopilot Elon Musk\n",
      "The Art of Fighting and the Pursuit of Excellence John Clarke\n",
      "Universal Artificial Intelligence, AIXI, and AGI Marcus Hutter\n",
      "Physics View of the Mind and Neurobiology John Hopfield\n",
      "Stack Overflow and Coding Horror Jeff Atwood\n",
      "Meaning of Life, the Universe, and Everything Manolis Kellis\n",
      "Simulation and Superintelligence Nick Bostrom\n",
      "Ex Machina, Devs, Annihilation, and the Poetry of Science Alex Garland\n",
      "Google Eric Schmidt\n",
      "Revolutionary Ideas in Science, Math, and Society Eric Weinstein\n",
      "Keras, Deep Learning, and the Progress of AI François Chollet\n",
      "Linear Algebra, Deep Learning, Teaching, and MIT OpenCourseWare Gilbert Strang\n",
      "Toward a Hybrid of Deep Learning and Symbolic AI Gary Marcus\n",
      "Chess, Deep Blue, AI, and Putin Garry Kasparov\n",
      "Adobe Research Gavin Miller\n",
      "Comma.ai, OpenPilot, and Autonomous Vehicles George Hotz\n",
      "3Blue1Brown and the Beauty of Mathematics Grant Sanderson\n",
      "OpenAI and AGI Greg Brockman\n",
      "Python Guido van Rossum\n",
      "Spotify Gustav Soderstrom\n",
      "Machine Learning, Recommender Systems, and the Future of AI Michael I. Jordan\n",
      "Cosmos, Carl Sagan, Voyager, and the Beauty of Science Ann Druyan\n",
      "Quantum Gravity and Einstein’s Unfinished Revolution Lee Smolin\n",
      "Ethereum, Cryptocurrency, and the Future of Money Vitalik Buterin\n",
      "Human-Robot Interaction and Reward Engineering Anca Dragan\n",
      "Leadership, Hard Work, Optimism and the Infinite Game Simon Sinek\n",
      "Physics of Consciousness and the Infinite Universe Roger Penrose\n",
      "AlphaGo, AlphaZero, and Deep Reinforcement Learning David Silver\n",
      "Evolution, Intelligence, Simulation, and Memes Richard Dawkins\n",
      "Geometric Unity and the Call for New Ideas, Leaders & Institutions Eric Weinstein\n",
      "Cellular Automata, Computation, and Physics Stephen Wolfram\n",
      "Generative Adversarial Networks (GANs) Ian Goodfellow\n",
      "Thousand Brains Theory of Intelligence Jeff Hawkins\n",
      "fast.ai Deep Learning Courses and Research Jeremy Howard\n",
      "Supersymmetry, String Theory and Proving Einstein Right Jim Gates\n",
      "Moore’s Law, Microprocessors, Abstractions, and First Principles Jim Keller\n",
      "Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI Judea Pearl\n",
      "Godel Machines, Meta-Learning, and LSTMs Juergen Schmidhuber\n",
      "AI Superpo Kai-Fu Lee\n",
      "Quantum Mechanics, String Theory, and Black Holes Leonard Susskind\n",
      "Lockheed Martin Keoki Jackson\n",
      "Life 3.0 Max Tegmark\n",
      "Concepts, Analogies, Common Sense & Future of AI Melanie Mitchell\n",
      "Reinforcement Learning, Planning, and Robotics Leslie Kaelbling\n",
      "Algorithmic Fairness, Bias, Privacy, and Ethics in Machine Learning Michael Kearns\n",
      "Vsauce Michael Stevens\n",
      "CPU times: user 5.6 s, sys: 143 ms, total: 5.74 s\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#loading in previous bytestreams, needs 2 since 2 podcast folders edit later\n",
    "with open(\"cucumber/Lex.bin\", \"rb\") as w: \n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))\n",
    "# with open(\"cucumber/first_doc.bin\", \"rb\") as w: #loading in\n",
    "#     new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "# docs1 += list(new_docs1.get_docs(nlp.vocab))\n",
    "for doc in docs1:\n",
    "    print(doc.user_data[\"title\"], doc.user_data[\"guest\"])\n",
    "# print(docs1[5].user_data[\"books\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each category of ents, generate csv files for ents corresponding to people\n",
    "\n",
    "att_to_csv(docs1, \"people\") \n",
    "# att_to_csv(docs1, \"places\")\n",
    "# att_to_csv(docs1, \"books\")\n",
    "# print(sorted(set([item for sublist in docs1 for item in sublist.user_data[\"people\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate megagraph for book ents\n",
    "edges = pd.read_csv(\"booksEdges.csv\", sep=',').drop(\"id\", axis=1).values.tolist()\n",
    "nodes = pd.read_csv('booksNodes.csv',sep=',').set_index(\"id\").to_dict(\"index\")\n",
    "\n",
    "source = [nodes.get(e[0])[\"name\"] for e in edges]\n",
    "target = [nodes.get(e[1])[\"name\"] for e in edges]\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':np.ones(len(source))})\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()\n",
    "\n",
    "#following are other example graphviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"Bible\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"Matthew Johnson\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['source']==\"Matthew Johnson\" )| (kg_df['source']==\"Ryan Hall\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[(kg_df['target']==\"Crime and Punishment\" )| (kg_df['target']==\"Atlas Shrugged\")], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "\n",
    "\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
