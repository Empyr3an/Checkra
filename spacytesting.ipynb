{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'nlp_implementation.py'\n",
    "alltext = nlp(text_fix(open(\"text_files/all_fix.txt\").read()))\n",
    "quotes = nlp(text_fix(open(\"text_files/gonnadelete.txt\").read()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints all sentences that have quotes\n",
    "for sent in alltext.sents:\n",
    "    if sent._.quote==True:\n",
    "        print(sent.text,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimportant sentences\n",
    "with open(\"gonnadelete.txt\", \"w+\") as w:\n",
    "    for sent in alltext.sents:\n",
    "        if len(sent) <6:\n",
    "            w.write(str(sent)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple subject/verb/object extraction\n",
    "with open(\"text_files/nounchunks2.txt\", \"w+\") as w:\n",
    "    for sent in alltext.sents:\n",
    "        w.write(sent.text)\n",
    "        w.write(\"\\n\")\n",
    "        for token in sent:\n",
    "            if token.pos_==\"NOUN\" or token.pos_==\"PROPN\":\n",
    "                w.write((str(token)+\" ****** \"))\n",
    "        w.write(\"\\n\")\n",
    "        for token in sent:\n",
    "            if token.pos_==\"VERB\":\n",
    "                w.write((str(token)+\" ****** \"))\n",
    "        w.write(\"\\n\\n\\n\")\n",
    "# find_noun(doc, \"plural\")\n",
    "# for i in range(len(quicktext)):\n",
    "#     print(quicktext[i].text, quicktext[i].pos_, quicktext[i].tag_, quicktext[i].dep_, quicktext[i].head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intent from multiple sentences sample\n",
    "\n",
    "doc = nlp(\"I have finsihed my soda. I want another one.I want another one.\")\n",
    "verbList = [(\"order\", \"want\", \"give\", \"make\"), (\"show\", \"find\")]\n",
    "dobjList = [(\"pizza\", \"pie\", \"pizzaz\"), (\"cola\", \"soda\")]\n",
    "substitutes = [(\"one\", \"it\", \"same\", \"more\")] # things to replace\n",
    "intent = {'verb':\",'dobj':\"}\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        if token.dep_ == 'dobj':\n",
    "            verbSyns = [item for item in verbList if token.head.text in item]\n",
    "            dobjSyns = [item for item in dobjList if token.text in item]\n",
    "            substitute = [item for item in substitutes if token.text in item]\n",
    "            if (dobjSyns != [] or substitute !=[]) and verbSyns !=[]:\n",
    "                intent['verb'] = verbSyns[0][0]\n",
    "            if dobjSyns !=[]:\n",
    "                intent['dobj'] = dobjSyns[0][0]\n",
    "            print(\"verbsy\", verbSyns, \"dobj\", dobjSyns, \"\")\n",
    "intentStr = intent['verb'] + intent['dobj'].capitalize()\n",
    "print(intentStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I want to place an order for a pizza\") #want intent if main words are not transitive verb or direct object\n",
    "dobj, tverb = \"\", \"\"\n",
    "\n",
    "for token in doc: #find direct object and corressponding transitive verb\n",
    "    if token.dep_ == \"dobj\":\n",
    "        dobj = token\n",
    "        tverb = token.head\n",
    "intentVerb = \"\"\n",
    "verbList = [\"want\", \"like\", \"need\", \"order\"]\n",
    "if tverb.text in verbList:# if tverb is what we are looking for\n",
    "    intentVerb = tverb\n",
    "else:\n",
    "    if tverb.head.dep_ == \"ROOT\":#if not, then we look at root of sentence for real intent verb\n",
    "        intentVerb = tverb.head\n",
    "intentObj = \"\"\n",
    "objList = ['pizza', 'cola']\n",
    "if dobj.text in objList:\n",
    "    intentObj = dobj\n",
    "else: \n",
    "    for child in dobj.children:\n",
    "        if child.dep_ == 'prep':\n",
    "            intentObj = list(child.children)[0]\n",
    "            break\n",
    "        elif child.dep_ == 'compound':\n",
    "            intentObj = child\n",
    "            break\n",
    "print(intentVerb.text + intentObj.text.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\") \n",
    "\n",
    "def extract_report(doc): \n",
    "    phrase = '' \n",
    "    for token in doc: \n",
    "#         print(token.text, token.pos_, spacy.explain(token.pos_), token.dep_, token.head)\n",
    "        if token.pos_ ==\"NUM\":\n",
    "#             print(token.text, token.head.text)\n",
    "            while True:\n",
    "                phrase += \" \"+token.text\n",
    "                token = token.head\n",
    "                if token not in list(token.head.lefts):\n",
    "                    phrase += \" \"+token.text\n",
    "                    break\n",
    "            \n",
    "            for i in token.subtree:\n",
    "                if i.dep_ == \"acl\":\n",
    "                    phrase +=\" \" + i.text\n",
    "                    break\n",
    "            break\n",
    "    phrase = phrase.strip()\n",
    "    print(phrase)\n",
    "#     print(doc[token.i].text)\n",
    "    while True:\n",
    "        token = doc[token.i].head\n",
    "\n",
    "        if token.pos_ != \"ADP\":\n",
    "            phrase = token.text +\" \"+ phrase\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            break\n",
    "    # print(phrase)\n",
    "    for tok in token.lefts:\n",
    "        if tok.dep_==\"nsubj\":\n",
    "    #         print([tok.text for tok in tok.lefts], tok.text)\n",
    "            phrase = \" \".join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' ' + phrase\n",
    "            break\n",
    "#     print(phrase) \n",
    "    return phrase\n",
    "doc = nlp(\"The company, whose profits reached a record high this year, largely attributed to changes in management, earned a total revenue of $4.26 million.\")\n",
    "\n",
    "phrase = extract_report(doc)\n",
    "print(phrase)\n",
    "\n",
    "# for i, token in enumerate(doc): \n",
    "#     print(token.text, token.pos_, spacy.explain(token.pos_), token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_destination(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.ent_type!=0 and token.ent_type ==\"GPE\":\n",
    "            while True: #keep iterating back from gpe to either word \"to\" or the root\n",
    "                token = token.head\n",
    "                if token.text == \"to\":\n",
    "                    return doc[i].text\n",
    "                if token.head == token: #if we reach root and no to is found\n",
    "                    return \"failure to determine\"\n",
    "    return \"failure to determine\"\n",
    "\n",
    "\n",
    "\n",
    "dot = nlp(\"I am going to the conference in Berlin\")\n",
    "print(\"It seems the user wants a ticket to \", det_destination(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_pattern(doc): #iterate through tokens to find subject + auxiliary + Root + object pattern\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == \"aux\" and doc[i+2].dep_ == \"ROOT\":\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_== \"dobj\":\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pos_pattern(doc): #iterate through tokens,check if dep_ pattern subject and object are both personal pronouns\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.tag_ != 'PRP': #prp is personal pronoun\n",
    "            return False\n",
    "        if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
    "            return False\n",
    "        if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
    "            return False\n",
    "        if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def pron_pattern(doc):\n",
    "    plural = [\"we\", \"us\", \"they\", \"them\"]\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"dobj\" and token.tag_ == \"PRP\":\n",
    "            if token.text in plural:\n",
    "                print(spacy.explain(token.pos_))\n",
    "                return 'plural'\n",
    "            else:\n",
    "                return 'regular'\n",
    "            \n",
    "    return \"not found\"\n",
    "def find_noun(sents, num):\n",
    "    if num ==\"plural\":\n",
    "        taglist = [\"NNS\", \"NNPS\"]\n",
    "    if num ==\"singular\":\n",
    "        taglist = [\"NN\", \"NNP\"]\n",
    "    for sent in reversed(sents):\n",
    "        for token in sent:\n",
    "            if token.tag_ in taglist:\n",
    "                current_noun = token.text\n",
    "                for w in token.children:\n",
    "                    if w.dep_ == \"det\":\n",
    "                        current_noun = w.text + \" \" + current_noun\n",
    "                return current_noun\n",
    "    return \"noun not found\"\n",
    "\n",
    "def gen_utterance(doc, noun):\n",
    "    sent = ''\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
    "            sent = doc[:i].text+' '+noun.lower()+' '+doc[i+1:len(doc)-2].text + 'too.'\n",
    "            return sent\n",
    "        \n",
    "    return \"failed to generate utterance\"\n",
    "\n",
    "\n",
    "\n",
    "doc = nlp(\"The symbols are clearly distinguishable. I can recognize them promptly.\")\n",
    "# displacy.serve(doc, style=\"dep\")\n",
    "# for tok in doc:\n",
    "#     print(tok.text, tok.tag_,spacy.explain(tok.tag_), \"****\", tok.head, tok.dep_, spacy.explain(tok.dep_))\n",
    "sents = list(doc.sents)\n",
    "response = ''\n",
    "noun = ''\n",
    "for i, sent in enumerate(sents):\n",
    "    if dep_pattern(sent) and pos_pattern(sent):\n",
    "        noun = find_noun(sents[:i], pron_pattern(sent))\n",
    "        if noun != \"noun not found\":\n",
    "            response = gen_utterance(sents[i], noun)\n",
    "            break\n",
    "print(response)          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"fruits\")[0]\n",
    "doc = nlp(\"I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know about this type of tree?\")\n",
    "similarity = {}\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    noun_span_list = [sent[j].text for j in range(len(sent)) if sent[j].pos_ == 'NOUN']\n",
    "    noun_span_str = \" \".join(noun_span_list)\n",
    "    noun_span_doc = nlp(noun_span_str)\n",
    "    similarity.update({i:token.similarity(noun_span_doc)})\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u'Google Search, often referred to as simply Google, is the mostused search engine nowadays. It handles a huge number of searches each day.') \n",
    "#second sample text\n",
    "doc2 = nlp(u'Microsoft Windows is a family of proprietary operating systems developed and sold by Microsoft. The company also produces a wide range of other software for desktops and servers.') \n",
    "#third sample text\n",
    "doc3 = nlp(u\"Titicaca is a large, deep, mountain lake in the Andes. It is known as the highest navigable lake in the world.\")\n",
    "docs = [doc1, doc2, doc3]\n",
    "spans = {}\n",
    "for j, doc in enumerate(docs):\n",
    "    ner_span = [doc[i].text for i in range(len(doc)) if doc[i].ent_type != 0]\n",
    "#     ner_span = [(e.text, e.label_, e.type_) for e in doc.ents]\n",
    "    print(ner_span)\n",
    "    ner_span = \" \".join(ner_span)\n",
    "    ner_span = nlp(ner_span)\n",
    "    spans.update({j:ner_span})\n",
    "print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))\n",
    "print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))\n",
    "print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('england is similar to anglo:',nlp(\"england\").similarity(nlp(\"india\")))\n",
    "print('india is similar to pakistan:',nlp(\"india\").similarity(nlp(\"pakistan\")))\n",
    "print('england is similar to pakistan:',nlp(\"england\").similarity(nlp(\"america\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I can surely promise it is worth your time.\")\n",
    "for tok in doc:\n",
    "    print(tok.text, tok.pos_,  tok.tag_,spacy.explain(tok.tag_), \"****\", tok.head, tok.dep_, spacy.explain(tok.dep_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ''\n",
    "for i, token in enumerate(doc):\n",
    "    if token.tag_ == \"PRP\" and doc[i+1].tag_ == \"MD\":\n",
    "        sent.append(doc[i+1].text.capitalize() + ' '+ doc[i].text +' '+ doc[i+2:].text)\n",
    "\n",
    "doc = nlp(sent)\n",
    "for i, token in (doc):\n",
    "    if token.tag_ == \"PRP\" and \n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk(doc):\n",
    "    chunk = \"\"\n",
    "    for i, tok in enumerate(doc):\n",
    "        if tok.dep_ == \"dobj\":\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I want to order a vetgetarian pizza\")\n",
    "for tok in doc:\n",
    "    print(tok.text, tok.tag_,spacy.explain(tok.tag_), \"****\", tok.dep_, spacy.explain(tok.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"we must overtake them\")\n",
    "doc2 = nlp(\"you must specify it\")\n",
    "\n",
    "for i in range(0, len(doc1)):\n",
    "    if doc1[i].dep_ == doc2[i].dep_:\n",
    "        print(doc1[i].text, doc2[i].text, doc1[i].dep_,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\"}, {\"DEP\":\"ROOT\"}]\n",
    "matcher.add(\"nsubjauxroot\", None, pattern)\n",
    "doc = nlp(\"We can overtake them\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(\"Span: \", span.text)\n",
    "    print(\"pos are\", start ,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ent1, ent2 = \"\", \"\"\n",
    "    prv_tok_dep, prv_tok_text = \"\",\"\"\n",
    "    prefix, modifier = \"\", \"\"\n",
    "    \n",
    "    for tok in nlp(sent):\n",
    "        if tok.dep_ != \"punct\":\n",
    "            print(\"\\nprevious tok dep:\", prv_tok_dep)\n",
    "            print(\"prev tok text: \", prv_tok_text)\n",
    "            print(\"pref:\", prefix)\n",
    "            print(\"modi:\", modifier)\n",
    "            \n",
    "            print(tok, \"...\", tok.dep_)\n",
    "            \n",
    "            if tok.dep_ == \"compound\" or tok.dep_.endswith(\"mod\") == True:\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "            if \"subj\" in tok.dep_:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\" \n",
    "            if \"obj\" in tok.dep_:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                print(\"fuck\", modifier, \"fuck\", prefix, \"fuck\", tok.text)\n",
    "#             else: \n",
    "#                 prv_tok_dep = \"\"\n",
    "#                 prv_tok_text = \"\" \n",
    "            \n",
    "        prv_tok_dep = tok.dep_\n",
    "        prv_tok_text = tok.text\n",
    "        print(\"\\nprevious tok dep:\", prv_tok_dep)\n",
    "        print(\"prev tok text: \", prv_tok_text)\n",
    "        print(\"pref:\", prefix)\n",
    "        print(\"modi:\", modifier)\n",
    "        print(\"\\n\\nentities:\", ent1, \"**** \",ent2)\n",
    "        print(\"****\")\n",
    "                    \n",
    "    \n",
    "    print(\"\\n\\nentities:\", ent1, \"**** \",ent2)\n",
    "    print(\"previous token stuff:\", prv_tok_dep, prv_tok_text)\n",
    "    print(\"pref:\", prefix)\n",
    "    print(\"modi:\", modifier)\n",
    "\n",
    "# get_entities(candidate_sentences[0].text)\n",
    "get_entities(\"the real estate drawdown process is governed by real estate standard d823\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Mary is playing a very old violin. She is very good at it.\"\"\"\n",
    "facts = nlp(text)\n",
    "facts._.coref_resolved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
