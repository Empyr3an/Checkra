{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import contractions\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def write_specific(url, folder):\n",
    "    main = \"https://www.happyscribe.com\"\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(requests.get(main+url).text, \"html.parser\")\n",
    "    epi_name = re.split(\" – |: \",soup.find(\"h1\").text)\n",
    "    transcript_text = soup.find(class_=\"hsp-episode-transcript-body\")\n",
    "    can_write = False\n",
    "    if not os.path.exists(folder+\"2\"):\n",
    "        os.makedirs(folder+\"2\")\n",
    "    lineNum = 0\n",
    "    with open(str(folder+\"2/\"+\"|\".join(epi_name).replace(\" \", \"_\")+\".txt\"), \"w+\") as w:\n",
    "        for para in transcript_text.find_all(class_=\"hsp-paragraph\"):\n",
    "            w.write(contractions.fix(para.text.split(\" \",1)[1]+\"\\n\"))\n",
    "            #trim advertisement for lex fridman podcasts, uncomment below comment above\n",
    "#             if (\"here is my conversation with\") in contractions.fix(para.text.lower()):\n",
    "#                 can_write = True\n",
    "#             if can_write == True or lineNum>7:\n",
    "#                 w.write(contractions.fix(para.text.split(\" \",1)[1])+\"\\n\")\n",
    "#             if (\"thank you for listening to this conversation with\") in contractions.fix(para.text.lower()):\n",
    "#                 can_write = False\n",
    "#             lineNum+=1\n",
    "                \n",
    "url=\"https://www.happyscribe.com/public/lex-fridman-podcast-artificial-intelligence-ai\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(open(\"htmlstuff.txt\").read(), 'html.parser')\n",
    "podcast_name = soup.find(class_=\"hsp-podcast-info\").find(\"h1\").text.replace(\" \",\"_\")\n",
    "\n",
    "for tag in soup.find_all(\"a\",class_=\"hsp-card-episode\"):\n",
    "    print(tag.attrs[\"href\"])\n",
    "    write_specific(tag.attrs[\"href\"], podcast_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = \"https://www.happyscribe.com\"\n",
    "url=\"/public/lex-fridman-podcast-artificial-intelligence-ai/145-matthew-johnson-psychedelics\"   \n",
    "    \n",
    "soup = BeautifulSoup(requests.get(main+url).text, \"html.parser\")\n",
    "epi_name = re.split(\" – |: \",soup.find(\"h1\").text)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "main = \"https://www.happyscribe.com\"\n",
    "url = \"/public/lex-fridman-podcast-artificial-intelligence-ai/145-matthew-johnson-psychedelics\"\n",
    "soup = BeautifulSoup(requests.get(main+url).text, \"html.parser\")\n",
    "\n",
    "transcript_text = re.split(\" – |: \",soup.find(\"h1\").text)\n",
    "print(transcript_text)\n",
    "can_write = False\n",
    "# if not os.path.exists(folder+\"1\"):\n",
    "#     os.makedirs(folder+\"1\")\n",
    "\n",
    "# with open(str(folder+\"1/\"+url.split(\"/\")[-1]+\".txt\"), \"w+\") as w:\n",
    "#     for para in transcript_text.find_all(class_=\"hsp-paragraph\"):\n",
    "# #             w.write(contractions.fix(para.text.split(\" \",1)[1]+\"\\n\"))\n",
    "#         #trim advertisement for lex fridman podcasts, uncomment below comment above\n",
    "#         if (\"here is my conversation with\") in contractions.fix(para.text.lower()):\n",
    "#             can_write = True\n",
    "#         if can_write == True:\n",
    "#             w.write(contractions.fix(para.text.split(\" \",1)[1])+\"\\n\")\n",
    "#         if (\"thank you for listening to this conversation with\") in contractions.fix(para.text.lower()):\n",
    "#             can_write = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.utils import ChromeType\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get(url)\n",
    "time.sleep(1)\n",
    "\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "\n",
    "no_of_pagedowns = 20\n",
    "\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    no_of_pagedowns-=1\n",
    "\n",
    "post_elems = browser.find_elements_by_class_name(\"hsp-card-episode\")\n",
    "\n",
    "for post in post_elems:\n",
    "    print(post.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amrlib\n",
    "import spacy\n",
    "amrlib.setup_spacy_extension()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('This is a test of the SpaCy extension. The test has multiple sentences.')\n",
    "graphs = doc._.to_amr()\n",
    "for graph in graphs:\n",
    "    print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amrlib\n",
    "stog = amrlib.load_stog_model()\n",
    "graphs = stog.parse_sents(['This is a test of the system.', 'This is a second sentence.'])\n",
    "for graph in graphs:\n",
    "    print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatternOmatic.api import find_patterns, Config\n",
    "\n",
    "sentences = open(\"books.txt\").read().split(\"\\n\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.max_generations = 150\n",
    "config.max_runs = 3\n",
    "\n",
    "patterns_found, _ = find_patterns(sentences)\n",
    "\n",
    "print(f'Patterns found: {patterns_found}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
