{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%run -i 'nlp_implementation.py' #imports packages and inits alltext\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# conf = SparkConf().setAppName(\"pyspark-shell\").setMaster('local[*]').set(\"spark.executor.memory\", \"10g\").set(\"spark.driver.memory\", \"10g\").set('spark.driver.maxResultSize', \"10G\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "with open(\"cucumber/all_doc.bin\", \"rb\") as w: #loading in all bytestreams of data\n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))\n",
    "with open(\"cucumber/first_doc.bin\", \"rb\") as w: #loading in\n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 += list(new_docs1.get_docs(nlp.vocab))\n",
    "print(len(docs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import wikipedia\n",
    "# from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "from difflib import SequenceMatcher\n",
    "import csv\n",
    "\n",
    "books_df = pd.read_csv('books_clean.csv')\n",
    "def is_book1(name, df=books_df): #worker\n",
    "#     print(\"trying\", name)\n",
    "    db, wiki, = False, False  \n",
    "    if name in df.title.values:\n",
    "        db = True\n",
    "    similar = [\"book\", \"volume\", \"novel\", \"work\", \"publication\", \"title\", \"treatise\"]\n",
    "    try:\n",
    "        summ = wikipedia.summary(name, sentences=3)\n",
    "        if any([x in summ.lower() for x in similar]):\n",
    "            wiki =True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if db or wiki:\n",
    "        print(name)\n",
    "        return(name, True)\n",
    "#     links = search(name)\n",
    "#     websites_matched = 0\n",
    "#     for l in links:\n",
    "#         if bool(re.search(\"amazon.*dp\", l)):\n",
    "#             websites_matched+=1\n",
    "#         if bool(re.search(\"books\\.google.*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "#             websites_matched+=1\n",
    "#         if bool(re.search(\"goodreads*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "#             websites_matched+=1\n",
    "#         if bool(re.search(\"barnesandnoble*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "#             websites_matched+=1\n",
    "#         if bool(re.search(\"penguinrandomhouse*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "#             websites_matched+=1\n",
    "\n",
    "#     if websites_matched>2 or sum([bool(re.search(\"book\", l.lower())) for l in links])>4:\n",
    "#         return (name, True) \n",
    "    \n",
    "    return (name, False)\n",
    "\n",
    "        \n",
    "def keep_stuff1(doc):\n",
    "    books, people, places = [], [], []\n",
    "    ents = doc.user_data[\"entis\"]\n",
    "    potential = [\"WORK_OF_ART\"]\n",
    "    \n",
    "    for ent, label in ents:\n",
    "        if label==\"PERSON\": #save people entities\n",
    "            people.append(ent)\n",
    "        elif label==\"LOC\" or label==\"GPE\": #save location entites\n",
    "            places.append(ent)\n",
    "    #parallel processing to verify books\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 20) as executor:\n",
    "        result = [executor.submit(is_book1, e[0]) for e in ents if e[1] in potential]\n",
    "    for future in concurrent.futures.as_completed(result):\n",
    "        print(future.result())\n",
    "        if future.result()[1]==True:\n",
    "            books.append(future.result()[0])\n",
    "    doc.user_data[\"places\"] = places\n",
    "    doc.user_data[\"people\"] = people\n",
    "    doc.user_data[\"books\"] = books\n",
    "#     doc.user_data[\"people\"] = [person[0] for person in ents if person[1]==\"PERSON\"]\n",
    "#     doc.user_data[\"place\"] = [place[0] for place in ents if place[1]==\"LOC\" or place[1]==\"GPE\"]\n",
    "#     doc.user_data[\"book\"] = [book[0] for book in ents if book[1] in potential]\n",
    "    \n",
    "    \n",
    "    return doc\n",
    "docs1 = [keep_stuff1(doc) for doc in docs1] #extract and save all entities by running above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def att_to_csv(docs, atts):\n",
    "    all_atts= sorted(set([item for sublist in docs for item in sublist.user_data[atts]])) #all books\n",
    "    all_attributes=dict([(x, [x]) for x in all_atts]) #dictionary of books to become key is base book, value are similar titles\n",
    "    i = 0\n",
    "#     print(len(all_attributes))\n",
    "#     similar_words implement dictionary to store similar words that were deleted\n",
    "    while i<len(all_attributes)-1: #remove similar or subwords\n",
    "        str1, str2 = all_atts[i], all_atts[i+1]\n",
    "        if str2 in str1 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)>len(str2)):\n",
    "            toRemove = all_attributes.get(str2) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str1)\n",
    "            del all_attributes[str2]\n",
    "            all_attributes[str1] = toRemove+toKeep\n",
    "        elif str1 in str2 or (SequenceMatcher(a=str1,b=str2).ratio()>.8 and len(str1)<=len(str2)):\n",
    "            toRemove = all_attributes.get(str1) #list of similar words moving\n",
    "            toKeep =  all_attributes.get(str2)\n",
    "            del all_attributes[str1]\n",
    "            all_attributes[str2] = toRemove+toKeep\n",
    "            \n",
    "        i+=1\n",
    "    print((all_attributes))    \n",
    "    new_dic = {} #reverse dict so that given any name, can find base\n",
    "    for k,v in all_attributes.items():\n",
    "        for x in v:\n",
    "            new_dic.setdefault(x,[]).append(k)\n",
    "    #now have dict of base books with values of similar titles\n",
    "        \n",
    "    all_attributes_dict = dict(zip(all_attributes.keys(), np.arange(len(all_attributes)))) #create dict for all entities\n",
    "    \n",
    "    \n",
    "    guests = [doc.user_data[\"guest\"] for doc in docs]\n",
    "    guests_dict = dict(zip(guests, np.arange(len(all_attributes), len(all_attributes)+len(guests))))#create dict for all main\n",
    "    \n",
    "#     #now create dict for all edges by going from every doc's mentions of attribute, and adding edge from guest to attribute\n",
    "    \n",
    "    i = len(guests) + len(all_attributes) #make keys for final edges\n",
    "\n",
    "#     print(all_attributes_dict.get((new_dic.get('A Course in Miracles')[0])))\n",
    "    edges = []\n",
    "    for doc in docs:\n",
    "        current_name = guests_dict.get(doc.user_data[\"guest\"]) #get graph id for each guest in doc\n",
    "        for mention in doc.user_data[atts]:\n",
    "            if new_dic.get(mention):\n",
    "#                 print(new_dic.get(mention))\n",
    "                edges.append((current_name, all_attributes_dict.get(new_dic.get(mention)[0]))) #from speaker to mention of base book\n",
    "#     print(edges)\n",
    "    edges_dict=dict(zip(edges, np.arange(i, i+len(edges))))\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(atts+\"Nodes.csv\", \"w+\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"id\", \"name\", \"group\"])\n",
    "        for ind, key in enumerate(all_attributes.keys()): #key is attriubte name, value is id\n",
    "            writer.writerow([ind, key, 2])\n",
    "        for key, value in guests_dict.items():\n",
    "            writer.writerow([value, key, 1]) # group 2 is entities, group 1 is speakers\n",
    "    with open(atts+\"Edges.csv\", \"w+\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"id\", \"source\", \"target\", \"value\"])\n",
    "        for key, value in edges_dict.items():\n",
    "            writer.writerow([value, key[0], key[1], 1])\n",
    "att_to_csv(docs1, \"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# att_to_csv(docs1, \"places\")\n",
    "# att_to_csv(docs1, \"books\")\n",
    "from collections import Counter\n",
    "\n",
    "all_attributes= Counter([item for sublist in docs1 for item in sublist.user_data[\"books\"]])\n",
    "for item in all_attributes.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "search = \"spongebob\"\n",
    "results = 10 # valid options 10, 20, 30, 40, 50, and 100\n",
    "page = requests.get(\"https://www.duckduckgo.com/?q=\"+search+\"&num=\"+str(results))\n",
    "# print(page.text)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "print(soup.prettify())\n",
    "links = soup.findAll(\"a\")\n",
    "for link in links :\n",
    "    link_href = link.get('href')\n",
    "    print(link_href)\n",
    "#     if \"url?q=\" in link_href and not \"webcache\" in link_href:\n",
    "#         print(link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
