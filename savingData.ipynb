{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%run -i 'nlp_implementation.py' #imports packages and inits alltext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark-shell\").setMaster('local[*]').set(\"spark.executor.memory\", \"10g\").set(\"spark.driver.memory\", \"10g\").set('spark.driver.maxResultSize', \"10G\")\n",
    "sc = SparkContext(conf=conf)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "with open(\"cucumber/all_doc.bin\", \"rb\") as w: #loading in all bytestreams of data\n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 = list(new_docs1.get_docs(nlp.vocab))\n",
    "with open(\"cucumber/first_doc.bin\", \"rb\") as w: #loading in\n",
    "    new_docs1 = DocBin(store_user_data=True).from_bytes(w.read())\n",
    "docs1 += list(new_docs1.get_docs(nlp.vocab))\n",
    "print(len(docs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import wikipedia\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "\n",
    "def worker(name, vidId, dest):\n",
    "    print(\"starting\", name, vidId)\n",
    "    ytid = \"--youtubeid=\"+ vidId\n",
    "    if not os.path.exists(dest.split(\"/\")[0]):\n",
    "        os.makedirs(dest.split(\"/\")[0])\n",
    "\n",
    "    while True:\n",
    "        subprocess.run([\"python3\", \"down.py\", ytid, \"--output\", dest], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        if os.stat(dest).st_size != 0:\n",
    "            print(\"done\")\n",
    "            break\n",
    "        print(\"going again\", name, vidId)\n",
    "\n",
    "books_df = pd.read_csv('books_clean.csv')\n",
    "def is_book1(name, df=books_df): #worker\n",
    "#     print(\"trying\", name)\n",
    "#     db, wiki, goog, searches = False, False, False, False\n",
    "    if name in df.title.values:\n",
    "        return (name,True)\n",
    "    try:\n",
    "        summ = wikipedia.summary(name, sentences=3)\n",
    "        if ent in summ:\n",
    "            (name,True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    links = search(name)\n",
    "    for l in links:\n",
    "        if bool(re.search(\"amazon.*dp\", l)):\n",
    "            return (name,True)\n",
    "        if bool(re.search(\"books\\.google.*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "            return (name,True)\n",
    "        if bool(re.search(\"goodreads*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "            return (name,True)\n",
    "        if bool(re.search(\"barnesandnoble*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "            return (name,True)\n",
    "        if bool(re.search(\"penguinrandomhouse*\"+name.replace(\" \", \"_\").lower(), l.lower())):\n",
    "            return (name,True)\n",
    "\n",
    "    if sum([bool(re.search(\"book\", l.lower())) for l in links])>4:\n",
    "        return (name,True) \n",
    "    \n",
    "    return (name, False)\n",
    "\n",
    "        \n",
    "def keep_stuff1(doc):\n",
    "    books, people, places = [], [], []\n",
    "    ents = doc.user_data[\"entis\"]\n",
    "    potential = [\"NORP\", \"ORG\", \"PRODUCT\", \"WORK_OF_ART\"]\n",
    "    \n",
    "    for ent, label in ents:\n",
    "        if label==\"PERSON\": #save people entities\n",
    "            people.append(ent)\n",
    "        elif label==\"LOC\" or label==\"GPE\": #save location entites\n",
    "            places.append(ent)\n",
    "    #parallel processing to verify books\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 1) as executor:\n",
    "        result = [executor.submit(is_book1, e[0]) for e in ents if e[1] in potential]\n",
    "    for future in concurrent.futures.as_completed(result):\n",
    "        if future.result()[1]==True:\n",
    "            books.append(futures.result()[0])\n",
    "    doc.user_data[\"books\"] = books\n",
    "    \n",
    "    return doc\n",
    "docs1 = [keep_stuff1(doc) for doc in docs1] #extract and save all entities by running above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from difflib import SequenceMatcher\n",
    "all_peeps = []\n",
    "for di in docs1:\n",
    "    all_peeps +=di.user_data[\"people\"]\n",
    "#     all_peeps.append(di.user_data[\"guest\"])\n",
    "# print(len(all_peeps))\n",
    "all_peeps = sorted(set(all_peeps))\n",
    "# all_peeps = [(peo, 1) for peo in all_peeps]\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(all_peeps)-1:\n",
    "    str1 = all_peeps[i]\n",
    "#     print(i)\n",
    "    str2 = all_peeps[i+1]\n",
    "    \n",
    "    if SequenceMatcher(a=str1,b=str2).ratio()>.8:\n",
    "        all_peeps.remove(str1) if len(str1)>len(str2) else all_peeps.remove(str2)\n",
    "    if str1 in str2:\n",
    "        all_peeps.remove(str1)\n",
    "    elif str2 in str1:\n",
    "        all_peeps.remove(str2)\n",
    "    i+=1\n",
    "\n",
    "mentions_dict = dict(zip(all_peeps, np.arange(len(all_peeps))))\n",
    "speakers = [guest.user_data[\"guest\"] for guest in docs1]\n",
    "speaker_dict = dict(zip(speakers, np.arange(len(all_peeps), len(all_peeps)+len(speakers))))\n",
    "\n",
    "one_speaker_mention = []\n",
    "for doc in docs1:\n",
    "    cur_id = speaker_dict.get(doc.user_data[\"guest\"])\n",
    "    for ment in doc.user_data[\"people\"]:\n",
    "        if mentions_dict.get(ment):\n",
    "            one_speaker_mention.append((cur_id, mentions_dict.get(ment), 1))\n",
    "\n",
    "i = len(all_peeps)+len(speakers)\n",
    "\n",
    "people_edges_dict = dict(zip(np.arange(i, i+len(one_speaker_mention)), one_speaker_mention))\n",
    "\n",
    "import csv\n",
    "with open('peopleNodes.csv', 'w+') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"id\", \"name\", \"group\"])\n",
    "    for key, value in mentions_dict.items():\n",
    "        writer.writerow([value, key, 2])\n",
    "    for key, value in speaker_dict.items():\n",
    "        writer.writerow([value, key, 1])\n",
    "with open('peopleEdges.csv', 'w+') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"id\", \"source\", \"target\", \"value\"])\n",
    "    for key, value in people_edges_dict.items():\n",
    "        writer.writerow([key, value[0], value[1], value[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time #next three cells manipulate saved entities and create dictionaries to csv\n",
    "all_places = []\n",
    "for di in docs1:\n",
    "    all_places +=di.user_data[\"places\"]\n",
    "all_places = sorted(set(all_places))\n",
    "# all_peeps = [(peo, 1) for peo in all_peeps]\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(all_places)-1:\n",
    "    str1 = all_places[i]\n",
    "#     print(i)\n",
    "    str2 = all_places[i+1]\n",
    "    \n",
    "    if SequenceMatcher(a=str1,b=str2).ratio()>.8:\n",
    "        all_places.remove(str1) if len(str1)>len(str2) else all_places.remove(str2)\n",
    "    if str1 in str2:\n",
    "        all_places.remove(str1)\n",
    "    elif str2 in str1:\n",
    "        all_places.remove(str2)\n",
    "    i+=1\n",
    "\n",
    "visited_dict = dict(zip(all_places, np.arange(len(all_places))))\n",
    "places = [guest.user_data[\"guest\"] for guest in docs1]\n",
    "place_dict = dict(zip(places, np.arange(len(all_places), len(all_places)+len(places))))\n",
    "\n",
    "one_place_mention = []\n",
    "for doc in docs1:\n",
    "    cur_id = place_dict.get(doc.user_data[\"guest\"])\n",
    "    for ment in doc.user_data[\"people\"]:\n",
    "        if place_dict.get(ment):\n",
    "            one_place_mention.append((cur_id, mentions_dict.get(ment), 1))\n",
    "\n",
    "i = len(all_places)+len(places)\n",
    "\n",
    "place_edges_dict = dict(zip(np.arange(i, i+len(one_place_mention)), one_place_mention))\n",
    "\n",
    "import csv\n",
    "with open('placeNodes.csv', 'w+') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"id\", \"name\", \"group\"])\n",
    "    for key, value in place_dict.items():\n",
    "        writer.writerow([value, key, 2])\n",
    "    for key, value in visited_dict.items():\n",
    "        writer.writerow([value, key, 1])\n",
    "with open('placeEdges.csv', 'w+') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"id\", \"source\", \"target\", \"value\"])\n",
    "    for key, value in place_edges_dict.items():\n",
    "        writer.writerow([key, value[0], value[1], value[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_books = []\n",
    "for di in docs1:\n",
    "    all_books +=di.user_data[\"books\"]\n",
    "all_books = sorted(set(all_books))\n",
    "i = 0\n",
    "\n",
    "while i < len(all_books)-1:\n",
    "    str1 = all_books[i]\n",
    "#     print(i)\n",
    "    str2 = all_books[i+1]\n",
    "    \n",
    "    if SequenceMatcher(a=str1,b=str2).ratio()>.8:\n",
    "        all_books.remove(str1) if len(str1)>len(str2) else all_books.remove(str2)\n",
    "    if str1 in str2:\n",
    "        all_books.remove(str1)\n",
    "    elif str2 in str1:\n",
    "        all_books.remove(str2)\n",
    "    i+=1\n",
    "\n",
    "read_book = dict(zip(all_books, np.arange(len(all_books))))\n",
    "books = [guest.user_data[\"guest\"] for guest in docs1]\n",
    "book_dict = dict(zip(books, np.arange(len(all_books), len(all_books)+len(books))))\n",
    "\n",
    "one_book_mention = []\n",
    "for doc in docs1:\n",
    "    cur_id = book_dict.get(doc.user_data[\"guest\"])\n",
    "    for ment in doc.user_data[\"people\"]:\n",
    "        if book_dict.get(ment):\n",
    "            one_book_mention.append((cur_id, book_dict.get(ment), 1))\n",
    "\n",
    "i = len(all_places)+len(places)\n",
    "\n",
    "book_edges_dict = dict(zip(np.arange(i, i+len(one_book_mention)), one_book_mention))\n",
    "\n",
    "import csv\n",
    "with open('bookNodes.csv', 'w+') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"id\", \"name\", \"group\"])\n",
    "    for key, value in book_dict.items():\n",
    "        writer.writerow([value, key, 2])\n",
    "    for key, value in read_book.items():\n",
    "        writer.writerow([value, key, 1])\n",
    "with open('bookEdges.csv', 'w+') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"id\", \"source\", \"target\", \"value\"])\n",
    "    for key, value in book_edges_dict.items():\n",
    "        writer.writerow([key, value[0], value[1], value[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
